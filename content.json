{"pages":[{"title":"about","text":"fdsagfsadfdfcxsacx fdsa f dsa fdsa f dsa fdsaf fdsafd dsaf f dsa f fdsa f dsa fds afdsa","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"film","text":"fdsafdsagvcxsavdasfd","link":"/film/index - 副本 (2).html"},{"title":"filmfdsafdas","text":"fdsafdsaf ds","link":"/film/index - 副本.html"},{"title":"film","text":"","link":"/film/index.html"},{"title":"旅行记","text":"重庆旅行记","link":"/book/index.html"},{"title":"nlp","text":"","link":"/nlp/index.html"},{"title":"travel","text":"","link":"/travel/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Hello World","text":"Welcome to Hexo,This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/10/08/hello-world/"},{"title":"","text":"机器人对你说：您的输入语句不通，语文是数学老师教的吧！背景:本文由下面一轮对话引出： 你：“过大嘎达发的撒旦法而我却人是” 机器人：“您的输入语句不通，语文是数学老师教的吧！” 那么机器人到底是怎么判断出一句话语句是否通顺呢？在客服场景中又有什么实际应用呢？ 方法:客服场景中有时需要判断用户的问句是否为乱输的无意义的，返回“请认真聊天，不要乱输”比返回万能回复会显得更加的智能，也可以用在智能质检上，检测答案或者问句的编写是否合理。 我们可以采用语言模型来判定句子的流畅度，并判断是否成句。 语言模型：通俗来讲，语言模型就是用来计算一组字成为句子的概率的模型，也就是判断一句话是否随机的概率。 另一个解释是：语言模型是假设一门语言所有可能的句子服从一个概率分布，每个句子出现的概率加起来是1，那么语言模型的任务就是预测每个句子在语言中出现的概率，对于语言中常见的句子，一个好的语言模型应该得到相对高的概率，对不合语法的句子，计算出的概率则趋近于零。 给定词语序列：$ S = W_1,W_2, …, W_k $，判断它成为句子的概率可表示为：$ P(S) = P(W_1,W_2,…,W_k) $ 我们使用$ P(w_3|w_1,w_2)$, 表示词$ w_1,w_2$后面是词$ w_3$的概率，现在我们需要计算一整句话的概率，例如“我,爱,北京,天安门”,需要计算“我爱北京天安门“同时出现 的概率，即$ P(我爱北京天安门) = P(我)P(爱|我)P(北京|我爱)P(天安门|我爱北京)$，计算这个概率的模型就叫做语言模型(Language Model)，随后即可用概率的链式法则来评估这一组随机变量的联合概率。一般的：$P(S) = P(w_1,w_2,…,w_k) = \\prod_{i=1}^k P(w_i|w_1,w_2,…,w_{k-1})$ 可这种方法会导致： 参数空间过大, 试想按上面方式计算$P(w_5 |w_1 ,w_2 ,w_3 ,w_4 )$，这里$w_i$都有一个词典大小取值的可能，记作|V|，则该模型的参数个数是$|V|^5$，而且这还不包含$P(w_4 | w_1, w_2, w_3)$的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法实用.(更为通俗的理解：还是以”我爱北京天安门“为例，$P(爱|我)$ 需要计算) 数据非常稀疏，导致最大似然概率接近0.(为什么最大似然接近0，本人在另一篇中对此做了解释，链接:) 统计语言模型在统计语言模型中为了减少参数数量，基于马尔可夫假设，采用2-gram模型，即我们可以认为下一个词的出现仅依赖于他前面的一个词，因此二元模型简化为： $P(S) = P(w_1)\\prod_{i=2}^kP(w_i|w_{i-1})$ 在此基础上进行拓展可演化为$n-gram$模型，假定文本中的每个词$w_i$只和其前面$n-1$个词相关，这时$P(w_i|w_1,w_2,…w_{i-1})=P(w_i|w_{i-n+1},w_{i-n+2},…w_{i-1})$ 这称为$n-gram$模型，其中$n$表示当前单词依赖它前面单词的个数，通常$n$取2、3、4,在本任务中，我们对$n=2，3$的情况做了实验，且对分词与未分词的情况分别做了实验。最终在$2-gram$且不分词的情况下效果达到最优。究其原因，可能是在本任务中，若是用户输的乱序文本则该句中基本每两个字之间都不成词。 $n-gram$ 模型的参数一般采用最大似然估计方法： $P(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)}{count(w_{i-1]})}$ 其中，count(x)表示词x在语料中出现的次数。 例子以真实句子为例对模型做一般说明。 例如，计算“今天能不能发货?”和”发货能今天不能”这两句话的ngram值。 以词级别为例，分词之后的句子为: 今天\\能\\不能\\发货 与发货\\能\\今天\\不能? 在本问题中，我选取人工客服聊天记录共100M作为训练预料，用来统计$n-gram$词频)各词的频率如下表所示: 词 词频 2-gram词 词频 今天 447 今天能 23 能 844 能不能 122 不能 481 不能发货 3 发货 501 发货? 20 ？ 3646 这里还需对句子加入起始符号与终止符号(我加的是start与end，不加的话会损失起始与终止的信息)，于是在此基础上加入$count(start今天)$与$count(?end)$ 于是2-gram分值:$P(今天能不能发货？)=P(start)P(今天|start)P(能|今天)P(不能|能)P(发货|不能)P(？|发货)P(end|？)$ 根据条件概率$P(A|B)=P(AB)/P(B)$,所以 $P(今天能不能发货？)=P(start今天)P(今天能)P(能不能)P(不能发货)(发货？)/（P(今天)P(能)P(不能)*P(发货)）$= 而在这个问题中，取得该词的概率可以近似为该词词频数/总词频数，即$P(今天)=count(今天)/总词频数=447/6066\\approx0.07$ 以此类推， 这样我们可以得到两个句子各自成句的概率： $P(今天能不能发货？)=0.000031$ $P(发货能今天不能？)=0.0000000053$ 故而“今天能不能发货”比“发货能今天不能”更加流畅。 进行预测概率时，除了统计语言模型外，目前使用较多的还有神经语言模型，这里就不详细展开了，大家感兴趣的话可以自己查查。 参考：https://blog.csdn.net/a635661820/article/details/43906731 https://zhuanlan.zhihu.com/machinelearningpku 《数学之美》第三章 统计语言模型 https://juejin.im/post/598c1941f265da3e190da56b","link":"/2019/07/23/什么是语言模型/"},{"title":"语言模型-从闲聊识别谈起","text":"研究背景:客服聊天记录中判断句子(这里是筛选出的含中文的句子，英文和符号及数字句子的处理可以在我的github中查看处理规则)是否为用户乱输的无意义的字词句 方法:采用语言模型来判定句子的流畅度，判断是否成句。 语言模型：通俗来讲，语言模型就是用来计算一组字成为句子的概率的模型，也就是判断一句话是否随机的概率。 另一个解释：语言模型是假设一门语言所有可能的句子服从一个概率分布，每个句子出现的概率加起来是1，那么语言模型的任务就是预测每个句子在语言中出现的概率，对于语言中常见的句子，一个好的语言模型应该得到相对高的概率，对不合语法的句子，计算出的概率则趋近于零。 给定词语序列：$ S = W_1,W_2, …, W_k $，判断它成为句子的概率可表示为：$ P(S) = P(W_1,W_2,…,W_k) $ 我们使用$ P(w_3|w_1,w_2)$, 表示词$ w_1,w_2$后面是词$ w_3$的概率，现在我们需要计算一整句话的概率，例如“我,爱,北京,天安门”,需要计算“我爱北京天安门“同时出现 的概率，即$ P(我爱北京天安门) = P(我)P(爱|我)P(北京|我爱)P(天安门|我爱北京)$，计算这个概率的模型就叫做语言模型(Language Model)，随后即可用概率的链式法则来评估这一组随机变量的联合概率。一般的：$P(S) = P(w_1,w_2,…,w_k) = \\prod_{i=1}^k P(w_i|w_1,w_2,…,w_{k-1})$ 可这种方法会导致： 参数空间过大, 试想按上面方式计算$P(w_5 |w_1 ,w_2 ,w_3 ,w_4 )$，这里$w_i$都有一个词典大小取值的可能，记作|V|，则该模型的参数个数是$|V|^5$，而且这还不包含$P(w_4 | w_1, w_2, w_3)$的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法实用.(更为通俗的理解：还是以”我爱北京天安门“为例，$P(爱|我)$ 需要计算) 数据非常稀疏，导致最大似然概率接近0.(为什么最大似然接近0，本人在另一篇中对此做了解释，链接:) 统计语言模型在统计语言模型中为了减少参数数量，基于马尔可夫假设，采用2-gram模型，我们可以认为下一个词的出现仅依赖于他前面的一个词，因此二元模型简化为： $P(S) = P(w_1)\\prod_{i=2}^kP(w_i|w_{i-1})$ 在此基础上进行拓展可演化为$n-gram$模型，假定文本中的每个词$w_i$只和其前面$n-1$个词相关，这时$P(w_i|w_1,w_2,…w_{i-1})=P(w_i|w_{i-n+1},w_{i-n+2},…w_{i-1})$ 这称为$n-gram$模型，其中$n$表示当前单词依赖它前面单词的个数，通常$n$取2、3、4,在本任务中，我们对$n=2，3$的情况做了实验，且对分词与未分词的情况分别做了实验。最终在$2-gram$且不分词的情况下效果达到最优。究其原因，可能是在本任务中，若是用户输的乱序文本则该句中基本每两个字之间都不成词。 $n-gram$ 模型的参数一般采用最大似然估计方法（这里可以看看数学之美的详细内容）： $P(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)}{count(w_{i-1]})}$ 其中，count(x)表示词x在语料中出现的次数。 例子以真实句子为例对模型做一般说明。 例如，计算“我的退款怎么还没收到?” 的ngram值。 以词级别为例，分词之后的句子为: 今天 能 不能 发货 ? 例如在训练语料中(语料的选择在《数学之美》中也有聊到，在本问题中，我选取人工客服聊天记录共11G作为训练预料，用来统计$n-gram$词频)各词的频率如下表所示: 词 词频 2-gram词 词频 今天 4472107 今天能 231074 能 8443281 能不能 1226651 不能 4818857 不能发货 24847 发货 5018585 发货? 204899 ？ 36461699 在这里顺便提一句文本预处理，我在这个任务中没有做去除标点符号与停用词的操作，因为我觉得在这个任务中标点和停用词是有其作用的。所以只去除了空格与换行符与成对的符号例如：&lt;&gt;《》“”(详细预处理及统计请参考我的github) 这里还需对句子加入起始符号与终止符号(我加的是start与end，不加的话会损失起始与终止的信息)，于是在此基础上加入$count(start今天)=$与$count(?end)=$ 于是2-gram分值:$P(今天能不能发货？)=P(start)P(今天|start)P(能|今天)P(不能|能)P(发货|不能)P(？|发货)P(end|？)$ 根据条件概率$P(A|B)=P(AB)/P(B)$,所以 $P(今天能不能发货？)=P(start今天)P(今天能)P(能不能)P(不能发货)(发货？)/（P(今天)P(能)P(不能)*P(发货)）$ 而在这个问题中，取得该词的概率可以近似为该词词频数/总词频数，即$P(今天)=count(今天)/总词频数$ $3-gram$ 在这里就不详细举例了，按公式来就好了 神经网络语言模型n-gram这种处理序列信息的方式，当n大于4时很难处理(google的模型好像用的是n=5的)而且提升效果很有限。另外n-gram并不能表示词与词之间的关联性。随着神经网络的兴起，基于神经网络的语言模型被发现，提供了另外一种语言模型思路。实际上，在深度学习中的时间序列模型，如RNN、LSTM、Bi-LSTM等，可以克服n-gram中无法关联距离较远的词的缺点。至于这些方法是如何克服ngram中n过大的问题，可以查看我后面的博客，对时间序列神经网络原理的理解。 one-hot在很久很久以前，大家使用计算机处理句子或文本时，使用简单明了的one-hot的形式来表示词，句子或篇章。one-hot方法构造简单，例如要表示一句话，首先统计文档中的全部词，将每个词作为一列，存在记为1，不存在记为0。上文中的例子”今天能不能发货?”可表示为： 今天 能 不能 发货 ? 今天 1 0 0 0 0 能 0 1 0 0 0 不能 0 0 1 0 0 发货 0 0 0 1 0 ？ 0 0 0 0 1 这种词的表示方式很明显维度爆炸大，有多少词就有多少维，刚才11G的句子会有61亿多维，若表示一句话则只有几个维度为1，非常稀疏，且无法表示词之间的相关关系。 优点就是可解释性强，模型简单。 Word2Vec为了克服one-hot表示的缺点 参考：https://blog.csdn.net/a635661820/article/details/43906731 https://zhuanlan.zhihu.com/machinelearningpku 《数学之美》第三章 统计语言模型 https://juejin.im/post/598c1941f265da3e190da56b","link":"/2018/09/15/语言模型-从闲聊识别谈起/"},{"title":"聚类算法详解-从文本聚类谈起","text":"摘要无监督的将相似句聚类，所以需要对聚类算法有比较深入的研究，单纯的调用sklearn无法满足工作需求，故对其进行实现。本文主要分三部分，第一部分描述对层次聚类的总结(因为这是工作中最终选取的算法)，第二部分描述kmeans算法(谈聚类怎么能不谈kmeans)，第三部分描述神经网络的聚类算法 算法详解层次聚类的步骤，终止条件分为两种，一种是聚到k类停止，一种是每两类之间的阈值大于某数。由于sklearn已经实现第一种方法，这里就不在赘述，主要分析一下第二种终止条件的实现及遇到的一些问题，全部代码请参考我的github。 算法原理层次聚类，顾名思义是将数据点分层进行聚类，主要有两种思路，一种是凝聚法，即算法开始时将每个节点设为一类，然后按照类间距离开始凝聚，知道凝聚满足终止条件。另一种是分裂法，即算法开始时所有数据点视为一类，然后开始分裂，直到分裂满足终止条件。 这里以凝聚法为例（分裂法原理一样，只是刚好相反），层次聚类涉及的主要问题在于类别之间的距离计算及聚类中心点的选择。 距离计算公式其他的距离计算公式在这里也不赘述，主流的距离度量方法可以参考sklearn的实现，sklearn的凝聚法实现了有6种距离度量，分别为“euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or ‘precomputed’。 由于我们是文本聚类，所以选择文本中最常用的余弦距离（选择余弦是有道理的，请自行google，或者等我填坑。。。）。 聚类中心点选择这里选择平均距离，因为经测试后，这种距离效果最好。 代码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215#层次聚类，两类之间使用平均距离作为类间距离from sklearn import preprocessingimport numpy as npimport heng_hierarchical_pre as h_preimport timeclass Hierarchical(object): \"\"\" 层次聚类 :param: min_dist -&gt;float32 聚类终止条件 :param: sub_node_id -&gt;list 存放聚完类的节点 :param: point_num -&gt;int 节点个数 :param: feature_num -&gt;int 节点特征数 \"\"\" def __init__(self, min_dist=0.63): self.min_dist = min_dist self.point_num = 0 self.feature_num = 0 self.distance_ori = None def binary_search1(self,nodes_pair_sort, x): ''' 二分查找 ''' left, right = 0, len(nodes_pair_sort)-1 if x&gt;=nodes_pair_sort[0]: return 0 if x&lt;=nodes_pair_sort[-1]: return -1 while left&lt;right: mid = int((left + right) / 2) if nodes_pair_sort[mid] &lt; x: right = mid if nodes_pair_sort[mid-1] &gt;= x: return mid-1 else: left = mid + 1 if nodes_pair_sort[mid+1] &lt;= x: return mid def binary_search2(self,nodes_pair_sort, x): ''' 二分查找 ''' left, right = 0, len(nodes_pair_sort)-1 if x&gt;=nodes_pair_sort[0][2]: return 0 if x&lt;=nodes_pair_sort[-1][2]: return -1 while left&lt;right: mid = int((left + right) / 2) if nodes_pair_sort[mid][2] &lt; x: right = mid if nodes_pair_sort[mid-1][2] &gt;= x: return mid else: left = mid + 1 if nodes_pair_sort[mid+1][2] &lt;= x: return mid+1 def fit(self, data): \"\"\" 训练模型，得到最终聚类结果 :param : data -&gt;np.array() 传入的训练数据 :param : distance_final -&gt;list 满足约束条件的节点对组成的列表 \"\"\" nodes = {} nodes_all_between_dist = {} data = np.array(data) self.point_num, self.feature_num = np.shape(data)[0], np.shape(data)[1] data = preprocessing.normalize(data,'l2') distance_big = np.zeros([self.point_num*2,self.point_num*2]) distance_ori = np.dot(data, data.T) self.distance_ori = distance_ori for i in range(self.point_num): distance_ori[i][i]=0 distance_half = np.triu(distance_ori) for i in range(self.point_num): nodes[i] = [i] nodes_all_between_dist[i] = distance_ori[i] min_dist = self.min_dist index_min_dist = np.where(distance_half &gt;= min_dist) nodes_pair = list()#存放全部满足条件节点对(left,right,dist) node_id_new = self.point_num#新节点id #满足条件节点对 for i in range(len(index_min_dist[0])): raw_distance, colume_distance = index_min_dist[0][i], index_min_dist[1][i] nodes_pair.append((raw_distance,colume_distance,distance_half[raw_distance][colume_distance])) nodes_before_del = set() nodes_pair_sort_1 = sorted(nodes_pair, key=lambda x:x[2], reverse=True) len_nodes_pair_sort = len(nodes_pair_sort_1) #每1000个放一个列表noedes_pair_sort = [[],[],...] print('排序前列表长度%d'%(len_nodes_pair_sort)) nodes_pair_sort = [] nodes_pair_range = np.arange(0, len_nodes_pair_sort, 16000) for index, value in enumerate(nodes_pair_range[:-1]): nodes_pair_sort.append(nodes_pair_sort_1[nodes_pair_range[index]:nodes_pair_range[index+1]]) nodes_pair_sort.append(nodes_pair_sort_1[nodes_pair_range[-1]:len_nodes_pair_sort]) count=0 distance_big[:self.point_num, :self.point_num] = distance_ori print('排序后列表长度%d'%(len(nodes_pair_sort))) while len(nodes_pair_sort)&gt;1: # start1 = time.time() # np.sort() # start2 = time.time() # print('排序耗时%f'%(start2-start1)) if nodes_pair_sort[0] == []: del nodes_pair_sort[0] #将每个组的最大的数加入nodes_pair_range_head中 nodes_pair_range_head = [] for value in nodes_pair_sort: # print(value) nodes_pair_range_head.append(value[0][2]) left, right = nodes_pair_sort[0][0][0], nodes_pair_sort[0][0][1] if left in nodes_before_del or right in nodes_before_del: del nodes_pair_sort[0][0] else: count+=1 nodes_before_del.add(left) nodes_before_del.add(right) # print('聚类的左右节点为%s,%s\\n'%(left,right)) # file_test.write('聚类的左右节点为%s,%s\\n'%(left,right)) nodes[node_id_new] = [] nodes[node_id_new] += nodes[left] nodes[node_id_new] += nodes[right] # print('新节点及其子节点为%s,%s'%(node_id_new,nodes[node_id_new])) start_insert = time.time() new_distance_1 = (distance_big[left]*len(nodes[left]) + distance_big[right]*len(nodes[right])) new_distance = new_distance_1/(len(nodes[left])+len(nodes[right])) # print(new_distance.shape) distance_big[:,node_id_new] = new_distance # print(distance_ori.shape) distance_big[node_id_new,:] = new_distance.reshape(1,len(new_distance)) end_insert = time.time() # print('计算节点耗时%f'%(end_insert - start_insert)) # print(distance_ori.shape) # print('节点有:%s'%' '.join([str(i) for i in nodes])) start_out = time.time() for node in nodes_before_del: distance_big[node,node_id_new] = 0 distance_big[node_id_new,node] = 0 nodes.pop(left) nodes.pop(right) node_new_dist = distance_big[:,node_id_new] count_insert = 0 start_getNewdict = time.time() for count_1,i in enumerate(node_new_dist): if i &gt;= min_dist: count_insert += 1 # print(nodes_pair_range_head) # print(len(nodes_pair_range_head)) nodes_pair_index = self.binary_search1(nodes_pair_range_head, i) # print(nodes_pair_index, i) index = self.binary_search2(nodes_pair_sort[nodes_pair_index], i) nodes_pair_sort[nodes_pair_index].insert(index,(node_id_new,count_1,i)) end_getNewdict = time.time() # print('查找插入索引耗时%f'%(end_getNewdict - start_getNewdict)) # print('插入次数%d'%count_insert) node_id_new += 1 end_out = time.time() # print('循环总耗时%f'%(end_out-start_out)) start_insert = time.time() labels = [nodes[i] for i in nodes] print('排序类别为%s'%len(nodes)) temp_labels = [] for index, value in enumerate(labels): if len(value)&gt;1: for j in value: temp_labels.append((j,index)) else: temp_labels.append((value[0],index)) labels = sorted(temp_labels,key=lambda x:x[0]) labels = [j for i,j in labels] end_insert = time.time() # print('排序节点耗时%f'%(end_insert - start_insert)) print('排序次数%d'%count) return labels def main(): start1 = time.time() data,_ = h_pre.main('udesk_test2.txt') print(data.shape) start2 = time.time() print('读取词向量耗时%.6s秒'%str(start2-start1)) print('*'*40) my1 = time.time() clust = Hierarchical(0.6) clust.fit(data) end1 = time.time() print('程序耗时%.6f秒'%float(end1-my1))if __name__ == '__main__': main()","link":"/2018/09/01/聚类算法详解-从文本聚类谈起/"},{"title":"最大似然-从朴素贝叶斯谈起","text":"","link":"/2018/10/09/最大似然-从朴素贝叶斯谈起/"},{"title":"语言模型-从闲聊识别谈起","text":"机器人对你说：您的输入语句不通，语文是数学老师教的吧！背景:本文由下面一轮对话引出： 你：“过大嘎达发的撒旦法而我却人是” 机器人：“您的输入语句不通，语文是数学老师教的吧！” 那么机器人到底是怎么判断出一句话语句是否通顺呢？在客服场景中又有什么实际应用呢？ 方法:客服场景中有时需要判断用户的问句是否为乱输的无意义的，返回“请认真聊天，不要乱输”比返回万能回复会显得更加的智能，也可以用在智能质检上，检测答案或者问句的编写是否合理。 我们可以采用语言模型来判定句子的流畅度，并判断是否成句。 语言模型：通俗来讲，语言模型就是用来计算一组字成为句子的概率的模型，也就是判断一句话是否随机的概率。 另一个解释是：语言模型是假设一门语言所有可能的句子服从一个概率分布，每个句子出现的概率加起来是1，那么语言模型的任务就是预测每个句子在语言中出现的概率，对于语言中常见的句子，一个好的语言模型应该得到相对高的概率，对不合语法的句子，计算出的概率则趋近于零。 给定词语序列：$ S = W_1,W_2, …, W_k $，判断它成为句子的概率可表示为：$ P(S) = P(W_1,W_2,…,W_k) $ 我们使用$ P(w_3|w_1,w_2)$, 表示词$ w_1,w_2$后面是词$ w_3$的概率，现在我们需要计算一整句话的概率，例如“我,爱,北京,天安门”,需要计算“我爱北京天安门“同时出现 的概率，即$ P(我爱北京天安门) = P(我)P(爱|我)P(北京|我爱)P(天安门|我爱北京)$，计算这个概率的模型就叫做语言模型(Language Model)，随后即可用概率的链式法则来评估这一组随机变量的联合概率。一般的：$P(S) = P(w_1,w_2,…,w_k) = \\prod_{i=1}^k P(w_i|w_1,w_2,…,w_{k-1})$ 可这种方法会导致： 参数空间过大, 试想按上面方式计算$P(w_5 |w_1 ,w_2 ,w_3 ,w_4 )$，这里$w_i$都有一个词典大小取值的可能，记作|V|，则该模型的参数个数是$|V|^5$，而且这还不包含$P(w_4 | w_1, w_2, w_3)$的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法实用.(更为通俗的理解：还是以”我爱北京天安门“为例，$P(爱|我)$ 需要计算) 数据非常稀疏，导致最大似然概率接近0.(为什么最大似然接近0，本人在另一篇中对此做了解释，链接:) 统计语言模型在统计语言模型中为了减少参数数量，基于马尔可夫假设，采用2-gram模型，即我们可以认为下一个词的出现仅依赖于他前面的一个词，因此二元模型简化为： $P(S) = P(w_1)\\prod_{i=2}^kP(w_i|w_{i-1})$ 在此基础上进行拓展可演化为$n-gram$模型，假定文本中的每个词$w_i$只和其前面$n-1$个词相关，这时$P(w_i|w_1,w_2,…w_{i-1})=P(w_i|w_{i-n+1},w_{i-n+2},…w_{i-1})$ 这称为$n-gram$模型，其中$n$表示当前单词依赖它前面单词的个数，通常$n$取2、3、4,在本任务中，我们对$n=2，3$的情况做了实验，且对分词与未分词的情况分别做了实验。最终在$2-gram$且不分词的情况下效果达到最优。究其原因，可能是在本任务中，若是用户输的乱序文本则该句中基本每两个字之间都不成词。 $n-gram$ 模型的参数一般采用最大似然估计方法： $P(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)}{count(w_{i-1]})}$ 其中，count(x)表示词x在语料中出现的次数。 例子以真实句子为例对模型做一般说明。 例如，计算“今天能不能发货?”和”发货能今天不能”这两句话的ngram值。 以词级别为例，分词之后的句子为: 今天\\能\\不能\\发货 与发货\\能\\今天\\不能? 在本问题中，我选取人工客服聊天记录共100M作为训练预料，用来统计$n-gram$词频)各词的频率如下表所示: 词 词频 2-gram词 词频 今天 447 今天能 23 能 844 能不能 122 不能 481 不能发货 3 发货 501 发货? 20 ？ 3646 这里还需对句子加入起始符号与终止符号(我加的是start与end，不加的话会损失起始与终止的信息)，于是在此基础上加入$count(start今天)$与$count(?end)$ 于是2-gram分值:$P(今天能不能发货？)=P(start)P(今天|start)P(能|今天)P(不能|能)P(发货|不能)P(？|发货)P(end|？)$ 根据条件概率$P(A|B)=P(AB)/P(B)$,所以 $P(今天能不能发货？)=P(start今天)P(今天能)P(能不能)P(不能发货)(发货？)/（P(今天)P(能)P(不能)*P(发货)）$= 而在这个问题中，取得该词的概率可以近似为该词词频数/总词频数，即$P(今天)=count(今天)/总词频数=447/6066\\approx0.07$ 以此类推， 这样我们可以得到两个句子各自成句的概率： $P(今天能不能发货？)=0.000031$ $P(发货能今天不能？)=0.0000000053$ 故而“今天能不能发货”比“发货能今天不能”更加流畅。 进行预测概率时，除了统计语言模型外，目前使用较多的还有神经语言模型，这里就不详细展开了，大家感兴趣的话可以自己查查。 参考：https://blog.csdn.net/a635661820/article/details/43906731 https://zhuanlan.zhihu.com/machinelearningpku 《数学之美》第三章 统计语言模型 https://juejin.im/post/598c1941f265da3e190da56b","link":"/2019/10/23/语言模型-从闲聊识别谈起-1/"},{"title":"book","text":"","link":"/2019/10/23/book/index/"},{"title":"重庆旅行日记","text":"","link":"/2019/10/23/book/重庆旅行日记/"},{"title":"聚类算法详解-从文本聚类谈起","text":"摘要无监督的将相似句聚类，所以需要对聚类算法有比较深入的研究，单纯的调用sklearn无法满足工作需求，故对其进行实现。本文主要分三部分，第一部分描述对层次聚类的总结(因为这是工作中最终选取的算法)，第二部分描述kmeans算法(谈聚类怎么能不谈kmeans)，第三部分描述神经网络的聚类算法 算法详解层次聚类的步骤，终止条件分为两种，一种是聚到k类停止，一种是每两类之间的阈值大于某数。由于sklearn已经实现第一种方法，这里就不在赘述，主要分析一下第二种终止条件的实现及遇到的一些问题，全部代码请参考我的github。 算法原理层次聚类，顾名思义是将数据点分层进行聚类，主要有两种思路，一种是凝聚法，即算法开始时将每个节点设为一类，然后按照类间距离开始凝聚，知道凝聚满足终止条件。另一种是分裂法，即算法开始时所有数据点视为一类，然后开始分裂，直到分裂满足终止条件。 这里以凝聚法为例（分裂法原理一样，只是刚好相反），层次聚类涉及的主要问题在于类别之间的距离计算及聚类中心点的选择。 距离计算公式其他的距离计算公式在这里也不赘述，主流的距离度量方法可以参考sklearn的实现，sklearn的凝聚法实现了有6种距离度量，分别为“euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or ‘precomputed’。 由于我们是文本聚类，所以选择文本中最常用的余弦距离（选择余弦是有道理的，请自行google，或者等我填坑。。。）。 聚类中心点选择这里选择平均距离，因为经测试后，这种距离效果最好。 代码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215#层次聚类，两类之间使用平均距离作为类间距离from sklearn import preprocessingimport numpy as npimport heng_hierarchical_pre as h_preimport timeclass Hierarchical(object): \"\"\" 层次聚类 :param: min_dist -&gt;float32 聚类终止条件 :param: sub_node_id -&gt;list 存放聚完类的节点 :param: point_num -&gt;int 节点个数 :param: feature_num -&gt;int 节点特征数 \"\"\" def __init__(self, min_dist=0.63): self.min_dist = min_dist self.point_num = 0 self.feature_num = 0 self.distance_ori = None def binary_search1(self,nodes_pair_sort, x): ''' 二分查找 ''' left, right = 0, len(nodes_pair_sort)-1 if x&gt;=nodes_pair_sort[0]: return 0 if x&lt;=nodes_pair_sort[-1]: return -1 while left&lt;right: mid = int((left + right) / 2) if nodes_pair_sort[mid] &lt; x: right = mid if nodes_pair_sort[mid-1] &gt;= x: return mid-1 else: left = mid + 1 if nodes_pair_sort[mid+1] &lt;= x: return mid def binary_search2(self,nodes_pair_sort, x): ''' 二分查找 ''' left, right = 0, len(nodes_pair_sort)-1 if x&gt;=nodes_pair_sort[0][2]: return 0 if x&lt;=nodes_pair_sort[-1][2]: return -1 while left&lt;right: mid = int((left + right) / 2) if nodes_pair_sort[mid][2] &lt; x: right = mid if nodes_pair_sort[mid-1][2] &gt;= x: return mid else: left = mid + 1 if nodes_pair_sort[mid+1][2] &lt;= x: return mid+1 def fit(self, data): \"\"\" 训练模型，得到最终聚类结果 :param : data -&gt;np.array() 传入的训练数据 :param : distance_final -&gt;list 满足约束条件的节点对组成的列表 \"\"\" nodes = {} nodes_all_between_dist = {} data = np.array(data) self.point_num, self.feature_num = np.shape(data)[0], np.shape(data)[1] data = preprocessing.normalize(data,'l2') distance_big = np.zeros([self.point_num*2,self.point_num*2]) distance_ori = np.dot(data, data.T) self.distance_ori = distance_ori for i in range(self.point_num): distance_ori[i][i]=0 distance_half = np.triu(distance_ori) for i in range(self.point_num): nodes[i] = [i] nodes_all_between_dist[i] = distance_ori[i] min_dist = self.min_dist index_min_dist = np.where(distance_half &gt;= min_dist) nodes_pair = list()#存放全部满足条件节点对(left,right,dist) node_id_new = self.point_num#新节点id #满足条件节点对 for i in range(len(index_min_dist[0])): raw_distance, colume_distance = index_min_dist[0][i], index_min_dist[1][i] nodes_pair.append((raw_distance,colume_distance,distance_half[raw_distance][colume_distance])) nodes_before_del = set() nodes_pair_sort_1 = sorted(nodes_pair, key=lambda x:x[2], reverse=True) len_nodes_pair_sort = len(nodes_pair_sort_1) #每1000个放一个列表noedes_pair_sort = [[],[],...] print('排序前列表长度%d'%(len_nodes_pair_sort)) nodes_pair_sort = [] nodes_pair_range = np.arange(0, len_nodes_pair_sort, 16000) for index, value in enumerate(nodes_pair_range[:-1]): nodes_pair_sort.append(nodes_pair_sort_1[nodes_pair_range[index]:nodes_pair_range[index+1]]) nodes_pair_sort.append(nodes_pair_sort_1[nodes_pair_range[-1]:len_nodes_pair_sort]) count=0 distance_big[:self.point_num, :self.point_num] = distance_ori print('排序后列表长度%d'%(len(nodes_pair_sort))) while len(nodes_pair_sort)&gt;1: # start1 = time.time() # np.sort() # start2 = time.time() # print('排序耗时%f'%(start2-start1)) if nodes_pair_sort[0] == []: del nodes_pair_sort[0] #将每个组的最大的数加入nodes_pair_range_head中 nodes_pair_range_head = [] for value in nodes_pair_sort: # print(value) nodes_pair_range_head.append(value[0][2]) left, right = nodes_pair_sort[0][0][0], nodes_pair_sort[0][0][1] if left in nodes_before_del or right in nodes_before_del: del nodes_pair_sort[0][0] else: count+=1 nodes_before_del.add(left) nodes_before_del.add(right) # print('聚类的左右节点为%s,%s\\n'%(left,right)) # file_test.write('聚类的左右节点为%s,%s\\n'%(left,right)) nodes[node_id_new] = [] nodes[node_id_new] += nodes[left] nodes[node_id_new] += nodes[right] # print('新节点及其子节点为%s,%s'%(node_id_new,nodes[node_id_new])) start_insert = time.time() new_distance_1 = (distance_big[left]*len(nodes[left]) + distance_big[right]*len(nodes[right])) new_distance = new_distance_1/(len(nodes[left])+len(nodes[right])) # print(new_distance.shape) distance_big[:,node_id_new] = new_distance # print(distance_ori.shape) distance_big[node_id_new,:] = new_distance.reshape(1,len(new_distance)) end_insert = time.time() # print('计算节点耗时%f'%(end_insert - start_insert)) # print(distance_ori.shape) # print('节点有:%s'%' '.join([str(i) for i in nodes])) start_out = time.time() for node in nodes_before_del: distance_big[node,node_id_new] = 0 distance_big[node_id_new,node] = 0 nodes.pop(left) nodes.pop(right) node_new_dist = distance_big[:,node_id_new] count_insert = 0 start_getNewdict = time.time() for count_1,i in enumerate(node_new_dist): if i &gt;= min_dist: count_insert += 1 # print(nodes_pair_range_head) # print(len(nodes_pair_range_head)) nodes_pair_index = self.binary_search1(nodes_pair_range_head, i) # print(nodes_pair_index, i) index = self.binary_search2(nodes_pair_sort[nodes_pair_index], i) nodes_pair_sort[nodes_pair_index].insert(index,(node_id_new,count_1,i)) end_getNewdict = time.time() # print('查找插入索引耗时%f'%(end_getNewdict - start_getNewdict)) # print('插入次数%d'%count_insert) node_id_new += 1 end_out = time.time() # print('循环总耗时%f'%(end_out-start_out)) start_insert = time.time() labels = [nodes[i] for i in nodes] print('排序类别为%s'%len(nodes)) temp_labels = [] for index, value in enumerate(labels): if len(value)&gt;1: for j in value: temp_labels.append((j,index)) else: temp_labels.append((value[0],index)) labels = sorted(temp_labels,key=lambda x:x[0]) labels = [j for i,j in labels] end_insert = time.time() # print('排序节点耗时%f'%(end_insert - start_insert)) print('排序次数%d'%count) return labels def main(): start1 = time.time() data,_ = h_pre.main('udesk_test2.txt') print(data.shape) start2 = time.time() print('读取词向量耗时%.6s秒'%str(start2-start1)) print('*'*40) my1 = time.time() clust = Hierarchical(0.6) clust.fit(data) end1 = time.time() print('程序耗时%.6f秒'%float(end1-my1))if __name__ == '__main__': main()","link":"/2018/09/01/book/聚类算法详解-从文本聚类谈起/"}],"tags":[{"name":"nlp","slug":"nlp","link":"/tags/nlp/"}],"categories":[{"name":"book","slug":"book","link":"/categories/book/"}]}