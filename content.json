{"pages":[{"title":"外面的世界","text":"最愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光，就令荧火一般，也可以在黑暗里发一点光，不必等候炬火。——鲁迅《热风·随感录四十一》 小时候，村子好大，梦想着有一天可以从村东头走到村西头。 长大后，却发现我们的世界好小，外面的世界好大。 慢慢长大，也在慢慢遗忘。在这里记录下我一些我认为重要的东西。希望能在垂垂暮年之际在这里可以回忆一生。","link":"/about/index.html"},{"title":"书单","text":"","link":"/book/index - 副本 (10) - 副本 - 副本.html"},{"title":"思君不见下渝州-重庆行","text":"如果现在拿出中国地图，选一个最想去的省去旅行，我想“四川”会是我内心的选择之一。提到四川，脑海里最先浮现出的竟然是李亚鹏版《笑傲江湖》里，青城派掌门余沧海的脸谱变脸邪功，真是童年阴影^_^。 趁着学校没什么事儿，实习的事情也告一段落，决定来一场说走就走的旅行。由于从“太原”直飞”重庆“的飞机很赶巧，第一站就选在了“重庆”，之后坐高铁去“成都”。 要说起对重庆的印象，脑海里便只剩两个字“火锅”。“火锅”算是我最爱吃的东西之一了。同学或者朋友来了，一般都会发起吃火锅的提议。尤其是在深秋或者飘着雪花的寒冬，萧瑟的秋风中，缓缓飘落的雪花旁，吃着热哄哄的火锅，身心都特别的舒服。听闻南方人吃火锅的小料是用香油的，这对 没去重庆对重庆的印象一直停留在“辣”这个字上，一直听说南方的火锅是蘸香油吃的。 重庆是一个","link":"/book/index - 副本 (10).html"},{"title":"书单","text":"","link":"/book/index - 副本 (11) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (12) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (13) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (14) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (15) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (16) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (17) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (18) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (19) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (20) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (21) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (22) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (23) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (24) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (25) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (26) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (27) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (28) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (29) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (3) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (3) - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (3).html"},{"title":"书单","text":"","link":"/book/index - 副本 (30) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (31) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (32) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (33) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (34) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (35) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (36) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (37) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (38) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (39) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (4) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (4) - 副本.html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (4).html"},{"title":"书单","text":"","link":"/book/index - 副本 (40) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (41) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (42) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (43) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (44) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (45) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (46) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (47) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (48) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (49) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (5) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (5) - 副本.html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (5).html"},{"title":"书单","text":"","link":"/book/index - 副本 (50) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (51) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (52) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (53) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (54) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (55) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (56) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (57) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (58) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (59) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (6) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (6) - 副本.html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (6).html"},{"title":"书单","text":"","link":"/book/index - 副本 (60) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (61) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (62) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (63) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (64) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (65) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (66) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (67) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (68) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (69) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (7) - 副本 - 副本.html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (7).html"},{"title":"书单","text":"","link":"/book/index - 副本 (70) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (71) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (72) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (73) - 副本 - 副本.html"},{"title":"书单","text":"","link":"/book/index - 副本 (8) - 副本 - 副本.html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (8).html"},{"title":"书单","text":"","link":"/book/index - 副本 (9) - 副本 - 副本.html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (9).html"},{"title":"Pandas相关操作","text":"Pandas相关操作Pandas 是python的一个数据分析包，在做NLP任务时可以极大的提高我们的处理效率，所以需要一些入门的知识。小道会在这章里自己涉及到的全部pandas操作。文章是用MD写的，大家可以根据右侧的目录查看自己感兴趣的操作。 pandas读取csv文件pandas读取csv文件使用函数read_csv,可以将csv文件读取为DataFrame。这里用示例详细演示。这里写一个csv文件，用作测试 12345678910with open(\"data/test_pandas_read.csv\", 'w', encoding='utf-8') as fout: fout.write(\"姓名,性别,年龄\\n\") fout.write(\"张一,男,21\\n\") fout.write(\"张二,男,22\\n\") fout.write(\"张三,男,23\\n\") fout.write(\"张四,男,24\\n\") fout.write(\"张五,男,25\\n\") fout.write(\"张六,男,26\\n\") fout.write(\"张七,男,27\\n\") fout.write(\"张八,男,28\\n\") 123import pandas as pdtrain_data = pd.read_csv(\"data/test_pandas_read.csv\")print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 说明一下几个参数 sep:分隔符，默认逗号 header:指定行数作为列名，默认为第1行，如果没有header行就设置为None index_col:用作行索引的列编号 names:用于结果的列名列表 skiprows：忽略的行数 nrows：读取的行数 123#这里使用123作列索引，把第一行也作为数据读取，只读取前6行train_data_test = pd.read_csv(\"data/test_pandas_read.csv\", sep=\",\", header=None, names=['1','2','3'], nrows=6)print(train_data_test) 1234567 1 2 30 姓名 性别 年龄1 张一 男 212 张二 男 223 张三 男 234 张四 男 245 张五 男 25 pandas 的基本DataFrame操作DataFrame 是一种二维的数据结构，非常接近于电子表格或者类似 mysql 数据库的形式。它的竖行称之为 columns，横行称之为 index，也就是说可以通过 columns 和 index 来确定一个主句的位置。上文的文件读入之后会成为一个DataFrame对象，下面列举常用的几个方法： 1print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 DataFrame的创建 通过列表创建 123456789tmp = [1,2,3,4]print(\"create with default index and column\")print(pd.DataFrame(tmp))print(\"create with own index and column\")print(pd.DataFrame(tmp, columns=[\"name\"], index=[\"num1\",\"num2\",\"num3\",\"num4\"]))#二维列表创建tmp_dim2 = [[1,2,3],[2,3,4]]print(\"create use dimension 2 list\")# print(pd.DataFrame(tmp_dim2) 12345678910111213create with default index and column 00 11 22 33 4create with own index and column namenum1 1num2 2num3 3num4 4create use dimension 2 list 通过numpy数组创建 1234567import numpy as nptmp = np.array([[1,2,3],[1,2,3]])print(pd.DataFrame(tmp))np.random.randint(12)tmp = np.reshape(np.random.random(12),(3,4))print(\"numpy array use random\")print(pd.DataFrame(tmp)) 12345678 0 1 20 1 2 31 1 2 3numpy array use random 0 1 2 30 0.408672 0.161733 0.583770 0.0816801 0.501434 0.156215 0.422717 0.0795052 0.149095 0.683209 0.984084 0.927738 通过字典创建 1print(pd.DataFrame({'name':[\"小道1\",\"小道2\"],'age':[23,24]})) 123 name age0 小道1 231 小道2 24 DataFrame的常用方法查看数据 查看前几行：head方法，默认5行 查看后几行：tail方法，默认5行 查看索引:index, column 查看数据:values，loc, iloc 1print(train_data.head()) 123456 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 25 1print(train_data.tail(3)) 1234 姓名 性别 年龄5 张六 男 266 张七 男 277 张八 男 28 123print(\"index of row\\n\",train_data.index)print(\"index of column\\n\",train_data.columns)print(\"data values\\n\",train_data.values) 12345678910111213index of row RangeIndex(start=0, stop=8, step=1)index of column Index([&apos;姓名&apos;, &apos;性别&apos;, &apos;年龄&apos;], dtype=&apos;object&apos;)data values [[&apos;张一&apos; &apos;男&apos; 21] [&apos;张二&apos; &apos;男&apos; 22] [&apos;张三&apos; &apos;男&apos; 23] [&apos;张四&apos; &apos;男&apos; 24] [&apos;张五&apos; &apos;男&apos; 25] [&apos;张六&apos; &apos;男&apos; 26] [&apos;张七&apos; &apos;男&apos; 27] [&apos;张八&apos; &apos;男&apos; 28]] 查看行数据与列数据 123print(\"查看列数据\\n%s\"%train_data[\"姓名\"])print(\"\\n查看行数据\\n%s\"%train_data[0:1])#这里查看行时只能用连续索引的形式，如果是单索引（train_data[0]）会报错 1234567891011121314查看列数据0 张一1 张二2 张三3 张四4 张五5 张六6 张七7 张八Name: 姓名, dtype: object查看行数据 姓名 性别 年龄0 张一 男 21 可以使用loc函数通过标签查看及选取数据 1print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 也可以使用iloc函数通过位置查看和选取数据,这也是小道最常用的取数据方式 1234tmp = train_data.iloc[0:2,0:2]print(\"取前两行两列的数据\\n%s\"%tmp)tmp = train_data.iloc[1,2]print(\"\\n取第二行的第3列的数据\\n%s\"%tmp) 1234567取前两行两列的数据 姓名 性别0 张一 男1 张二 男取第二行的第3列的数据22 对数据的统计：describe函数 1234print(\"字符串信息统计\\n%s\"%train_data[\"姓名\"].describe())print(\"数值信息统计\\n%s\"%train_data[\"年龄\"].describe())print(\"count is \"%train_data[\"年龄\"].describe().count())print(\"max num is \"%train_data[\"年龄\"].describe().max()) 123456789101112131415161718字符串信息统计count 8unique 8top 张二freq 1Name: 姓名, dtype: object数值信息统计count 8.00000mean 24.50000std 2.44949min 21.0000025% 22.7500050% 24.5000075% 26.25000max 28.00000Name: 年龄, dtype: float64count is max num is 对列进行排序 1print(train_data.sort_index(axis=1,ascending=True)) 123456789 姓名 年龄 性别0 张一 21 男1 张二 22 男2 张三 23 男3 张四 24 男4 张五 25 男5 张六 26 男6 张七 27 男7 张八 28 男 1train_data[\"年龄\"].sort() 123456---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-65-7a17c7a44327&gt; in &lt;module&gt;----&gt; 1 train_data[&quot;年龄&quot;].sort() 123456~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in __getattr__(self, name) 5065 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5066 return self[name]-&gt; 5067 return object.__getattribute__(self, name) 5068 5069 def __setattr__(self, name, value): 1AttributeError: &apos;Series&apos; object has no attribute &apos;sort&apos; 缺失值的处理1在pandas中，缺失值处理也是常见问题。","link":"/book/index.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (10) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (10) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (11) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (11) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (12) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (12) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (13) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (13) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (14) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (14) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (15) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (15) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (16) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (16) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (17) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (17) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (18) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (18) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (19) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (19) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (2) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (2).html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (20) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (20) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (21) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (21) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (22) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (23) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (25) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (24) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (26) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (27) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (28) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (29) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (3) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (3) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (30) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (31) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (32) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (33) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (34) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (35) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (36) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (37) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (38) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (39) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (4) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (4) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (40) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (41) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (42) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (43) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (44) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (45) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (46) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (47) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (48) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (49) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (5) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (5) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (50) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (51) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (52) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (53) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (54) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (55) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (56) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (57) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (58) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (59) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (6) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (6) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (60) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (61) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (62) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (63) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (64) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (65) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (66) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (67) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (68) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (69) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (7) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (7) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (70) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (71) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (72) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (73) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (74) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (75) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (8) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (8) - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (9) - 副本 - 副本.html"},{"title":"幸福终点站","text":"","link":"/film/index - 副本 (9) - 副本.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Pandas相关操作","text":"Pandas 是python的一个数据分析包，在做NLP任务时可以极大的提高我们的处理效率，所以需要一些入门的知识。小道会在这章里自己涉及到的全部pandas操作。文章是用MD写的，大家可以根据右侧的目录查看自己感兴趣的操作。 pandas读取csv文件pandas读取csv文件使用函数read_csv,可以将csv文件读取为DataFrame。这里用示例详细演示。这里写一个csv文件，用作测试 12345678910with open(\"data/test_pandas_read.csv\", 'w', encoding='utf-8') as fout: fout.write(\"姓名,性别,年龄\\n\") fout.write(\"张一,男,21\\n\") fout.write(\"张二,男,22\\n\") fout.write(\"张三,男,23\\n\") fout.write(\"张四,男,24\\n\") fout.write(\"张五,男,25\\n\") fout.write(\"张六,男,26\\n\") fout.write(\"张七,男,27\\n\") fout.write(\"张八,男,28\\n\") 123import pandas as pdtrain_data = pd.read_csv(\"data/test_pandas_read.csv\")print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 说明一下几个参数 sep:分隔符，默认逗号 header:指定行数作为列名，默认为第1行，如果没有header行就设置为None index_col:用作行索引的列编号 names:用于结果的列名列表 skiprows：忽略的行数 nrows：读取的行数 123#这里使用123作列索引，把第一行也作为数据读取，只读取前6行train_data_test = pd.read_csv(\"data/test_pandas_read.csv\", sep=\",\", header=None, names=['1','2','3'], nrows=6)print(train_data_test) 1234567 1 2 30 姓名 性别 年龄1 张一 男 212 张二 男 223 张三 男 234 张四 男 245 张五 男 25 pandas 的基本DataFrame操作DataFrame 是一种二维的数据结构，非常接近于电子表格或者类似 mysql 数据库的形式。它的竖行称之为 columns，横行称之为 index，也就是说可以通过 columns 和 index 来确定一个主句的位置。上文的文件读入之后会成为一个DataFrame对象，下面列举常用的几个方法： 1print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 DataFrame的创建 通过列表创建 123456789tmp = [1,2,3,4]print(\"create with default index and column\")print(pd.DataFrame(tmp))print(\"create with own index and column\")print(pd.DataFrame(tmp, columns=[\"name\"], index=[\"num1\",\"num2\",\"num3\",\"num4\"]))#二维列表创建tmp_dim2 = [[1,2,3],[2,3,4]]print(\"create use dimension 2 list\")# print(pd.DataFrame(tmp_dim2) 12345678910111213create with default index and column 00 11 22 33 4create with own index and column namenum1 1num2 2num3 3num4 4create use dimension 2 list 通过numpy数组创建 1234567import numpy as nptmp = np.array([[1,2,3],[1,2,3]])print(pd.DataFrame(tmp))np.random.randint(12)tmp = np.reshape(np.random.random(12),(3,4))print(\"numpy array use random\")print(pd.DataFrame(tmp)) 12345678 0 1 20 1 2 31 1 2 3numpy array use random 0 1 2 30 0.408672 0.161733 0.583770 0.0816801 0.501434 0.156215 0.422717 0.0795052 0.149095 0.683209 0.984084 0.927738 通过字典创建 1print(pd.DataFrame({'name':[\"小道1\",\"小道2\"],'age':[23,24]})) 123 name age0 小道1 231 小道2 24 DataFrame的常用方法查看数据 查看前几行：head方法，默认5行 查看后几行：tail方法，默认5行 查看索引:index, column 查看数据:values，loc, iloc 1print(train_data.head()) 123456 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 25 1print(train_data.tail(3)) 1234 姓名 性别 年龄5 张六 男 266 张七 男 277 张八 男 28 123print(\"index of row\\n\",train_data.index)print(\"index of column\\n\",train_data.columns)print(\"data values\\n\",train_data.values) 12345678910111213index of row RangeIndex(start=0, stop=8, step=1)index of column Index([&apos;姓名&apos;, &apos;性别&apos;, &apos;年龄&apos;], dtype=&apos;object&apos;)data values [[&apos;张一&apos; &apos;男&apos; 21] [&apos;张二&apos; &apos;男&apos; 22] [&apos;张三&apos; &apos;男&apos; 23] [&apos;张四&apos; &apos;男&apos; 24] [&apos;张五&apos; &apos;男&apos; 25] [&apos;张六&apos; &apos;男&apos; 26] [&apos;张七&apos; &apos;男&apos; 27] [&apos;张八&apos; &apos;男&apos; 28]] 查看行数据与列数据 123print(\"查看列数据\\n%s\"%train_data[\"姓名\"])print(\"\\n查看行数据\\n%s\"%train_data[0:1])#这里查看行时只能用连续索引的形式，如果是单索引（train_data[0]）会报错 1234567891011121314查看列数据0 张一1 张二2 张三3 张四4 张五5 张六6 张七7 张八Name: 姓名, dtype: object查看行数据 姓名 性别 年龄0 张一 男 21 可以使用loc函数通过标签查看及选取数据 1print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 也可以使用iloc函数通过位置查看和选取数据,这也是小道最常用的取数据方式 1234tmp = train_data.iloc[0:2,0:2]print(\"取前两行两列的数据\\n%s\"%tmp)tmp = train_data.iloc[1,2]print(\"\\n取第二行的第3列的数据\\n%s\"%tmp) 1234567取前两行两列的数据 姓名 性别0 张一 男1 张二 男取第二行的第3列的数据22 对数据的统计：describe函数 1234print(\"字符串信息统计\\n%s\"%train_data[\"姓名\"].describe())print(\"数值信息统计\\n%s\"%train_data[\"年龄\"].describe())print(\"count is \"%train_data[\"年龄\"].describe().count())print(\"max num is \"%train_data[\"年龄\"].describe().max()) 123456789101112131415161718字符串信息统计count 8unique 8top 张二freq 1Name: 姓名, dtype: object数值信息统计count 8.00000mean 24.50000std 2.44949min 21.0000025% 22.7500050% 24.5000075% 26.25000max 28.00000Name: 年龄, dtype: float64count is max num is 对列进行排序 1print(train_data.sort_index(axis=1,ascending=True)) 123456789 姓名 年龄 性别0 张一 21 男1 张二 22 男2 张三 23 男3 张四 24 男4 张五 25 男5 张六 26 男6 张七 27 男7 张八 28 男 1train_data[\"年龄\"].sort() 123456---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-65-7a17c7a44327&gt; in &lt;module&gt;----&gt; 1 train_data[&quot;年龄&quot;].sort() 123456~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in __getattr__(self, name) 5065 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5066 return self[name]-&gt; 5067 return object.__getattribute__(self, name) 5068 5069 def __setattr__(self, name, value): 1AttributeError: &apos;Series&apos; object has no attribute &apos;sort&apos; 缺失值的处理1在pandas中，缺失值处理也是常见问题。","link":"/2018/10/21/Pandas基础/"},{"title":"三体读后感","text":"第一次接触《三体》这本书还是大学的时候。大家一起打dota，一起看小说。我们宿舍8个人里居然有4个是网络小说迷。这本书就是听其中一个舍友讲起，讲了差不多有1个小时，依稀还记得四维空间，水滴和舰队。当时便对三体的世界产生了极大的兴趣。但也一直没有完整的看完小说。每次总是看几页便读不下去了，前面的红岸计划和三体游戏真心有些枯燥。 写论文的日子实在是无聊，又想起了这本书。于是边写论文边看《三体》，刚开始还是看着很迷惑。什么三体游戏，秦始皇、哥白尼的，这都是在说啥啊，说好的外星人入侵呢。 直到出现了第二红岸基地，出现了超出我想象的智子。才真正的感受到了三体的魅力。读完之后不得不佩服作者的构思。从文革时期的被批斗物理学家叶文洁向外星文明发射坐标到最终众志成城守护地球，环环相扣，引人入胜。 谈一谈读完之后最深的几个感受： 故事发生的很合理。文革时期确实国家有加大对外星文明的探索，这时候真有可能接受到了外星的回复。 人性探究很深刻。当被迫害，对国家失望对地球失望时，会作何选择。当舰队开始在太空流浪，自认为地球已经毁灭，自己成了唯一的地球生命，法律已经不能约束行为的时候，自己如果是舰长又会做出什么抉择。 生存本质的探究。自然界中总是强者为王，强者可以毫无顾忌的对弱者进行无情的蹂躏。在人类经历的昏暗年代，可能因为人种种族而被灭杀，可能因为瞅了某个贵族一眼而被灭杀，也可能打杂一辈子甚至没有埋葬亲人的钱。但还是到了现在的和平年代，美好的时代，努力就有收获的时代。我们是幸运的一代。同样，宇宙中强的生命也不能对弱者进行肆意打击，需要宇宙法律的维护，格局真大。","link":"/2018/10/23/三体/"},{"title":"哪吒之魔童降世","text":"我是一个很喜欢看动漫的人，看着国漫一步一步的崛起，油然而生的自豪感在内心荡漾。 在影院看完的第一感受就是，这部动漫电影可以写入中国动漫史了，是一部具有里程碑意义的作品。在特效、音乐和剧情上都做到了超越前人，给国产动漫树立了一个极高的标杆，也让我非常期待封神系列电影。 其实国漫特别爱打情怀牌，不管是大圣还是白蛇抑或是这次的哪吒都是大家耳熟能详的角色，大家内心对这些人物都有了固话的印象。就像这次的小哪吒，敖广出来时心里总觉得哪吒会抽他的经，然后龙王发怒，淹了澄塘观，然后李靖大神出来救场。 这次把敖广塑造成了纯真善良的太子，申公豹依旧背黑锅。但和封神不同的是，申公豹也有他自己的信条，从他的身世角度来看，他走到这步都是别人对他扭曲的认识造成的。非常的具有理性和逻辑性，非常好。电影中的每一个人物塑造都很饱满，包括吐泡泡的小妖和守门的两个封印小妖，形象设计也极具中国风。 几个印象深刻的场景: 哪吒为了救人却搞得鸡飞狗跳，被众人误解时。如果全世界都抛弃你，你该如何做抉择，我想在这里应该有很多观众希望哪吒黑化吧。 和敖广在海边踢毽子，然后扭扭捏捏，小心翼翼的送上自己生日贴的时候。仿佛看到了患得患失的年轻的自己。 “他是我儿”，生命在亲情面前显得那么脆弱，虽然剧情在意料之中，但还是控制不住的泪目了。 最好的朋友原却要毁灭我的世界时。正义必胜！","link":"/2019/09/26/哪吒/"},{"title":"","text":"就电影类型来说，我是喜欢看悬疑电影的。开局一起案件，紧接着便是一次次突如其来的推理，使人很容易进入剧情当中，结局也总是使人意犹未尽。 从某种程度来说《幸福终点站》和悬疑电影是有些类似的。通篇的谜题是男主来到纽约的目的，在电影中表现为一个随身携带的花生罐。我一直在猜编剧会往这个小罐子里放什么东西最终会带来意想不到的结局。随着男主性格的展现，是一个很乐观、无畏的人。在国家主权动荡时，也没有自怨自艾，一边关注祖国动态，一边积极的在候机厅里生活下去。 有几个细节很打动人： 发现整理推车可以赚到硬币时，难以抑制的内心喜悦，开心的吃着汉堡推着车，原来幸福真的可以很简单。 看着办公室里的电视播放自己国家动荡时，那种想帮忙却又无能为力的失落感，很让人动容。 英文不好，使用双语杂志逐词对照学语言时。扪心自问，我是真的做不到能在这种情况下耐心的学习。 女主说她一直在等一个电话时。感情的事情谁又能说的清楚，有时候放不下的其实只是内心不愿相信的执念。 其实到最后，谜底已经变得不那么重要了。是为了完成父亲的遗愿也好，是为了心中的梦想也好。过程有时候真的比结果更重要。","link":"/2019/11/12/幸福终点站/"},{"title":"最大似然-从朴素贝叶斯谈起","text":"","link":"/2018/10/09/最大似然-从朴素贝叶斯谈起/"},{"title":"浅谈文本表示","text":"文本在计算机中进行训练，首先得表示成一串向量，如何Embedding文本，是NLP中一个重要的研究方向。这里理一下目前的进展","link":"/2019/09/01/浅谈文本表示/"},{"title":"语言模型-从闲聊识别谈起","text":"背景:本文由下面一轮对话引出： 你：“过大嘎达发的撒旦法而我却人是” 机器人：“您的输入语句不通，语文是数学老师教的吧！” 那么机器人到底是怎么判断出一句话语句是否通顺呢？在客服场景中又有什么实际应用呢？ 方法:客服场景中有时需要判断用户的问句是否为乱输的无意义的，返回“请认真聊天，不要乱输”比返回万能回复会显得更加的智能，也可以用在智能质检上，检测答案或者问句的编写是否合理。 我们可以采用语言模型来判定句子的流畅度，并判断是否成句。 语言模型：通俗来讲，语言模型就是用来计算一组字成为句子的概率的模型，也就是判断一句话是否随机的概率。 另一个解释是：语言模型是假设一门语言所有可能的句子服从一个概率分布，每个句子出现的概率加起来是1，那么语言模型的任务就是预测每个句子在语言中出现的概率，对于语言中常见的句子，一个好的语言模型应该得到相对高的概率，对不合语法的句子，计算出的概率则趋近于零。 给定词语序列：$ S = W_1,W_2, …, W_k $，判断它成为句子的概率可表示为：$ P(S) = P(W_1,W_2,…,W_k) $ 我们使用$ P(w_3|w_1,w_2)$, 表示词$ w_1,w_2$后面是词$ w_3$的概率，现在我们需要计算一整句话的概率，例如“我,爱,北京,天安门”,需要计算“我爱北京天安门“同时出现 的概率，即$ P(我爱北京天安门) = P(我)P(爱|我)P(北京|我爱)P(天安门|我爱北京)$，计算这个概率的模型就叫做语言模型(Language Model)，随后即可用概率的链式法则来评估这一组随机变量的联合概率。一般的：$P(S) = P(w_1,w_2,…,w_k) = \\prod_{i=1}^k P(w_i|w_1,w_2,…,w_{k-1})$ 可这种方法会导致： 参数空间过大, 试想按上面方式计算$P(w_5 |w_1 ,w_2 ,w_3 ,w_4 )$，这里$w_i$都有一个词典大小取值的可能，记作|V|，则该模型的参数个数是$|V|^5$，而且这还不包含$P(w_4 | w_1, w_2, w_3)$的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法实用.(更为通俗的理解：还是以”我爱北京天安门“为例，$P(爱|我)$ 需要计算) 数据非常稀疏，导致最大似然概率接近0.(为什么最大似然接近0，本人在另一篇中对此做了解释，链接:) 统计语言模型在统计语言模型中为了减少参数数量，基于马尔可夫假设，采用2-gram模型，即我们可以认为下一个词的出现仅依赖于他前面的一个词，因此二元模型简化为： $P(S) = P(w_1)\\prod_{i=2}^kP(w_i|w_{i-1})$ 在此基础上进行拓展可演化为$n-gram$模型，假定文本中的每个词$w_i$只和其前面$n-1$个词相关，这时$P(w_i|w_1,w_2,…w_{i-1})=P(w_i|w_{i-n+1},w_{i-n+2},…w_{i-1})$ 这称为$n-gram$模型，其中$n$表示当前单词依赖它前面单词的个数，通常$n$取2、3、4,在本任务中，我们对$n=2，3$的情况做了实验，且对分词与未分词的情况分别做了实验。最终在$2-gram$且不分词的情况下效果达到最优。究其原因，可能是在本任务中，若是用户输的乱序文本则该句中基本每两个字之间都不成词。 $n-gram$ 模型的参数一般采用最大似然估计方法： $P(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)}{count(w_{i-1]})}$ 其中，count(x)表示词x在语料中出现的次数。 例子以真实句子为例对模型做一般说明。 例如，计算“今天能不能发货?”和”发货能今天不能”这两句话的ngram值。 以词级别为例，分词之后的句子为: 今天\\能\\不能\\发货 与发货\\能\\今天\\不能? 在本问题中，我选取人工客服聊天记录共100M作为训练预料，用来统计$n-gram$词频)各词的频率如下表所示: 词 词频 2-gram词 词频 今天 447 今天能 23 能 844 能不能 122 不能 481 不能发货 3 发货 501 发货? 20 ？ 3646 这里还需对句子加入起始符号与终止符号(我加的是start与end，不加的话会损失起始与终止的信息)，于是在此基础上加入$count(start今天)$与$count(?end)$ 于是2-gram分值:$P(今天能不能发货？)=P(start)P(今天|start)P(能|今天)P(不能|能)P(发货|不能)P(？|发货)P(end|？)$ 根据条件概率$P(A|B)=P(AB)/P(B)$,所以 $P(今天能不能发货？)=P(start今天)P(今天能)P(能不能)P(不能发货)(发货？)/（P(今天)P(能)P(不能)*P(发货)）$= 而在这个问题中，取得该词的概率可以近似为该词词频数/总词频数，即$P(今天)=count(今天)/总词频数=447/6066\\approx0.07$ 以此类推， 这样我们可以得到两个句子各自成句的概率： $P(今天能不能发货？)=0.000031$ $P(发货能今天不能？)=0.0000000053$ 故而“今天能不能发货”比“发货能今天不能”更加流畅。 进行预测概率时，除了统计语言模型外，目前使用较多的还有神经语言模型，这里就不详细展开了，大家感兴趣的话可以自己查查。 参考：https://blog.csdn.net/a635661820/article/details/43906731 https://zhuanlan.zhihu.com/machinelearningpku 《数学之美》第三章 统计语言模型 https://juejin.im/post/598c1941f265da3e190da56b","link":"/2019/08/20/语言模型-从闲聊识别谈起/"},{"title":"思君不见下渝州-重庆行","text":"如果现在拿出中国地图，选一个最想去的地方去旅行，我想“四川”会是我内心的选择之一。提到四川，脑海里最先浮现出的竟然是李亚鹏版《笑傲江湖》里，青城派掌门余沧海的脸谱变脸邪功，真的是童年阴影啊。其次是带领我们走向小康的小平主席在与撒切尔夫人的谈话影像。 趁着学校没什么事儿，实习的事情也告一段落，来一场潇洒的说走就走的旅行。由于从“太原”直飞”重庆“的飞机很赶巧，第一站就选在了“重庆”，之后坐高铁去“成都”。 要说起对重庆的印象，脑海里便只剩两个字“火锅”。“火锅”算是我最爱吃的东西之一了。同学或者朋友来了，一般都会发起吃火锅的提议。尤其是在深秋或者飘着雪花的寒冬，萧瑟的秋风中，缓缓飘落的雪花旁，吃着热哄哄的火锅，身心都特别的舒服。听闻南方人吃火锅的小料是用香油的，这对 没去重庆对重庆的印象一直停留在“辣”这个字上，一直听说南方的火锅是蘸香油吃的。","link":"/2018/10/26/重庆日记/"},{"title":"","text":"2019.10.1 阿尔山引子去阿尔山是一个巧合。今年四月份以后，由于一些事情，周末都过得很焦虑，也没有出去旅行。事情在八月底告一段落，就决定国庆节的时候好好出去玩耍一番。节假日出行我还是更倾向于旅行团的，因为车票和机票真的是太难搞定了。本来是决定去湖南一带的，但和朋友商量之后改成了去内蒙（最后他没去，这个坑货）。于是，开始了一个人的阿尔山之行。 山和水其实在到达阿尔山之前，我完全没有听说过这个地方，只是觉得旅行团的宣传照很漂亮，有靛蓝的天空，青绿的草原，奔驰的骏马。满怀憧憬的开始了这次的阿尔山之行。 抱团旅行的坏处就是行程几乎全在路上，每天只有大概3个小时左右可以在景点游玩。导致一趟7天的旅行下来，心里却觉得空落落的。只留下了4个值得珍藏在记忆深处的旅行记忆。杜鹃湖，草原奔驰，呼伦贝尔湖，夜晚的星空。 虽然名为“阿尔山”，但经过导游的讲解得知：阿尔山全名哈伦阿尔山，是蒙语音译，汉语的意思的热的圣水。可想而知是水更加的出名。","link":"/2019/11/12/阿尔山/"},{"title":"文本分类","text":"相较于短文本，处理长文本时，需要更加精细的处理，这里针对不同长度的文本，分别做研究。 短文本分类数据集介绍在网上找了好久，最终决定使用DataFontain平台的互联网新闻情感分析竞赛数据集来作为这次短文本分类的训练用数据集。竞赛简介如下：本赛题目标为在庞大的数据集中精准的区分文本的情感极性，情感分为正中负三类。面对浩如烟海的新闻信息，精确识别蕴藏在其中的情感倾向，对舆情有效监控、预警及疏导，对舆情生态系统的良性发展有着重要的意义。 数据集示例如下： 123import pandas as pdtrain_data = pd.read_csv(\"data/Train_DataSet.csv\")train_data.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } id title content 0 7a3dd79f90ee419da87190cff60f7a86 问责领导(上黄镇党委书记张涛，宣国才真能一手遮天吗？) 这几天看了有人举报施某某的贴子，经与举报人联系证实，是宣某当天中午请举报人和枪手喝酒后，晚上... 1 7640a5589bc7486ca199eeeb38af79dd 江歌事件:教会孩子，善良的同时更要懂得保护自己! 过去一年的江歌悲剧，这几日再次刷屏：住在东京都中野区的中国女留学生江歌，收留了被前男友陈世锋... 2 8c5bda93e4ba401f90a0faa5b28fe57f 绝味鸭脖广告\"开黄腔\"引众怒 \"双11\"这么拼值吗? “双11”1600亿的销售额让中国乃至全世界感到震惊，为此，不仅有不惜欠债百万的“剁手党”，... 3 1aa777fed31a4b8a9d866f05b5477557 央视曝光!如东一医药企业将槽罐车改成垃圾车，夜间偷排高浓度废水 2016年以来，如东高新区一家叫做拜瑞生物医药的企业竟然将装废水的槽罐车伪装成洒水车，常年在... 4 6c67ac55360340258e157f3710ebae6c 恶劣至极，央视都曝光了!南通如东一医药企业将槽罐车改成洒水车，夜间偷排高浓度废水...丢大发了! 新闻资讯•民生热点•城市生活•南通人都在看浏览更多资讯请关注微信公众号:南通传媒网昨晚央视一... 在本章中，我们只使用新闻标题作为训练数据，来判断新闻的类别，在下章的长文本分类时在使用新闻的content，最终结合使用所有的数据得到最终的分数。目前初赛快结束了，分数最高的是0.8231。在本文写完之后看下最终的分数能否达到近似的水平，由于我没有测试集的label，所以使用训练集划分的分数作为最终的分数，差别应该不会很大。那我们现在就开始系统的文本分类探究吧！ 数据预处理数据预处理是做NLP必不可少的流程之一。由于是NLP的第一篇文章，所以这里对预处理做详细的说明，比较几种预处理的结果。常用的预处理流程如下： 根据数据集的情况，有时需要去掉数字或者链接或者符号之类的 ，或者对数字做归一化处理等 分词 去停用词一般而言，这3步就是对文本的预处理流程了、现在开始实战。 12train_label = pd.read_csv(\"data/Train_DataSet_Label.csv\", header=0)train_label.head(5) id label 0 7a3dd79f90ee419da87190cff60f7a86 2 1 7640a5589bc7486ca199eeeb38af79dd 1 2 8c5bda93e4ba401f90a0faa5b28fe57f 2 3 1aa777fed31a4b8a9d866f05b5477557 2 4 6c67ac55360340258e157f3710ebae6c 2 先把文本和label对应一下。 12345678910id_label = {i[0]:i[1] for i in train_label.values}id_train = {i[0]:i[1] for i in train_data.values}# print(id_label)text_label = [(id_train[i], id_label[i]) for i in id_label if i in id_train]print(\"\\n\".join([str(i) for i in text_label[:5]]))print(\"the length of text_label\",len(text_label))print(\"the length of train_data\",len(train_data.values))#将text和label分别作为listtext_all = [text for text,_ in text_label]label_all = [label for _,label in text_label] 1234567(&apos;问责领导(上黄镇党委书记张涛，宣国才真能一手遮天吗？)&apos;, 2)(&apos;江歌事件:教会孩子，善良的同时更要懂得保护自己!&apos;, 1)(&apos;绝味鸭脖广告&quot;开黄腔&quot;引众怒 &quot;双11&quot;这么拼值吗?&apos;, 2)(&apos;央视曝光!如东一医药企业将槽罐车改成垃圾车，夜间偷排高浓度废水&apos;, 2)(&apos;恶劣至极，央视都曝光了!南通如东一医药企业将槽罐车改成洒水车，夜间偷排高浓度废水...丢大发了!&apos;, 2)the length of text_label 7340the length of train_data 7345 可以看出有5条是id没对应起来，这5条咱就不要了，对大局没影响。 接下来需要对文本做预处理。这里我们做个对比试验，一种是经过预处理的，另一种是未预处理，查看预处理对各种算法的影响。这里需要特别指出的是：现在的深度模型基本不需要对文本做预处理了，很多情况下基于字的特征比词的特征具有更高的准确率，后续小道会针对这个问题作详细的对比实验与说明。但预处理作为NLP的重要基础之一，还是需要了解一下滴本章预处理流程为：分词，去标点，去停用词（也可以将标点写在停用词表内一步到位，这样分开是因为有时候标点和停用词的去除处理不同） 分词（这里使用python最常用的分词包jieba）123import jiebatext_data = [jieba.lcut(str(sentence)) for sentence in text_all]print(text_data[-10:]) 1[[&apos;帮&apos;, &apos;女郎&apos;, &apos;在&apos;, &apos;行动&apos;], [&apos;稻城&apos;, &apos;亚丁&apos;, &apos;+&apos;, &apos;318&apos;, &apos;川藏线&apos;, &apos;+&apos;, &apos;拉萨&apos;, &apos;+&apos;, &apos;青藏线&apos;, &apos;，&apos;, &apos;你&apos;, &apos;心动&apos;, &apos;了&apos;, &apos;吗&apos;, &apos;？&apos;], [&apos;大良&apos;, &apos;、&apos;, &apos;龙江&apos;, &apos;、&apos;, &apos;陈村&apos;, &apos;4&apos;, &apos;所&apos;, &apos;学校&apos;, &apos;9&apos;, &apos;月&apos;, &apos;开学&apos;, &apos;!&apos;, &apos;顺德&apos;, &apos;未来&apos;, &apos;两年&apos;, &apos;力&apos;, &apos;...&apos;], [&apos;最&apos;, &apos;容易&apos;, &apos;受到&apos;, &apos;甲醛&apos;, &apos;侵害&apos;, &apos;的&apos;, &apos;五类&apos;, &apos;人&apos;, &apos;，&apos;, &apos;有&apos;, &apos;你&apos;, &apos;吗&apos;, &apos;？&apos;, &apos;_&apos;, &apos;污染&apos;], [&apos;2019&apos;, &apos;年&apos;, &apos;1&apos;, &apos;月&apos;, &apos;事故&apos;, &apos;伤亡&apos;, &apos;月&apos;, &apos;报&apos;], [&apos;珊瑚&apos;, &apos;裸尾鼠&apos;, &apos;：&apos;, &apos;首个&apos;, &apos;因&apos;, &apos;全球&apos;, &apos;气候&apos;, &apos;变暖&apos;, &apos;灭绝&apos;, &apos;的&apos;, &apos;哺乳动物&apos;], [&apos;独居&apos;, &apos;老人&apos;, &apos;做饭&apos;, &apos;忘关&apos;, &apos;火&apos;, &apos; &apos;, &apos;南通&apos;, &apos;志愿者&apos;, &apos;及时发现&apos;, &apos;转危为安&apos;], [&apos;被&apos;, &apos;生意&apos;, &apos;上&apos;, &apos;的&apos;, &apos;人&apos;, &apos;给&apos;, &apos;利用&apos;, &apos;合同诈骗&apos;, &apos;，&apos;, &apos;诈骗&apos;, &apos;三十万&apos;, &apos;够判&apos;, &apos;多少&apos;, &apos;年&apos;, &apos;-&apos;, &apos;-&apos;, &apos;在&apos;, &apos;..._&apos;, &apos;律师&apos;, &apos;365&apos;], [&apos;奎山&apos;, &apos;汽贸&apos;, &apos;城&apos;, &apos;去年&apos;, &apos;那场&apos;, &apos;火灾&apos;, &apos;，&apos;, &apos;调查&apos;, &apos;情况&apos;, &apos;报告&apos;, &apos;出来&apos;, &apos;了&apos;, &apos;！&apos;], [&apos;曝光&apos;, &apos;台&apos;, &apos;•&apos;, &apos;调查&apos;, &apos;｜&apos;, &apos;市场&apos;, &apos;消防通道&apos;, &apos;被&apos;, &apos;长期&apos;, &apos;霸占&apos;, &apos;？&apos;, &apos;事情&apos;, &apos;并非&apos;, &apos;想象&apos;, &apos;的&apos;, &apos;那样&apos;]] 去除标点符号可以使用string自带的punctuation获取英文标点和zhon包的punctuation获取中文标点 12345678910111213141516171819202122import stringimport zhon.hanzi as hanprint(\"English puctuation is :\",string.punctuation)print(\"Chinese puctuation is :\",han.punctuation)#需要查询操作，所以这里选用查找时间复杂度O(1)的集合punc = set()for i in string.punctuation: punc.add(i)for i in han.punctuation: punc.add(i)#这里举个例子sentence = \"fjdk sal fgjd,.,/,/,/.,.[]\"sentence_after = ''.join(word for word in sentence if word not in punc)print(\"sentence remove punctuation is :\",sentence_after)text_remove_punc = []for sentence in text_data: tmp = [] for word in sentence: if word not in punc: tmp.append(word) text_remove_punc.append(tmp)print(text_remove_punc[:10]) 1234English puctuation is : !&quot;#$%&amp;&apos;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~Chinese puctuation is : ＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､ 、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·！？｡。sentence remove punctuation is : fjdk sal fgjd[[&apos;问责&apos;, &apos;领导&apos;, &apos;上&apos;, &apos;黄镇&apos;, &apos;党委书记&apos;, &apos;张涛&apos;, &apos;宣国&apos;, &apos;才&apos;, &apos;真能&apos;, &apos;一手遮天&apos;, &apos;吗&apos;], [&apos;江歌&apos;, &apos;事件&apos;, &apos;教会&apos;, &apos;孩子&apos;, &apos;善良&apos;, &apos;的&apos;, &apos;同时&apos;, &apos;更要&apos;, &apos;懂得&apos;, &apos;保护&apos;, &apos;自己&apos;], [&apos;绝味&apos;, &apos;鸭&apos;, &apos;脖&apos;, &apos;广告&apos;, &apos;开&apos;, &apos;黄腔&apos;, &apos;引&apos;, &apos;众怒&apos;, &apos; &apos;, &apos;双&apos;, &apos;11&apos;, &apos;这么&apos;, &apos;拼值&apos;, &apos;吗&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;垃圾车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;], [&apos;恶劣&apos;, &apos;至极&apos;, &apos;央视&apos;, &apos;都&apos;, &apos;曝光&apos;, &apos;了&apos;, &apos;南通&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;洒水车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢&apos;, &apos;大发&apos;, &apos;了&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;南通&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢脸&apos;], [&apos;粉丝&apos;, &apos;爆料&apos;, &apos;五洲&apos;, &apos;国际&apos;, &apos;无锡&apos;, &apos;项目&apos;, &apos;涉嫌&apos;, &apos;诈骗&apos;, &apos;非法&apos;, &apos;集资&apos;], [&apos;年内&apos;, &apos;约&apos;, &apos;10&apos;, &apos;起&apos;, &apos;锂电&apos;, &apos;重组&apos;, &apos;失败&apos;, &apos; &apos;, &apos;资本&apos;, &apos;对&apos;, &apos;高&apos;, &apos;估值&apos;, &apos;收购&apos;, &apos;说&apos;, &apos;不&apos;], [&apos;男子&apos;, &apos;梦想&apos;, &apos;一夜&apos;, &apos;暴富&apos;, &apos;持&apos;, &apos;水泥块&apos;, &apos;砸机&apos;], [&apos;北京&apos;, &apos;多家&apos;, &apos;法院&apos;, &apos;供暖&apos;, &apos;纠纷&apos;, &apos;案件&apos;, &apos;主体&apos;, &apos;为&apos;, &apos;供暖费&apos;, &apos;追缴&apos;, &apos;山海&apos;, &apos;网&apos;]] 去除停用词这里提供我自己常用的停用词表，大家可以根据不同的需求自己设定停用词表 123456789101112with open(\"data/stopwords\", encoding='utf-8') as fin: stopwords = set() for i in fin: stopwords.add(i.strip())text_after = []for sentence in text_remove_punc: tmp = [] for word in sentence: if word not in stopwords: tmp.append(word) text_after.append(tmp)print(text_after[:10]) 1[[&apos;问责&apos;, &apos;领导&apos;, &apos;上&apos;, &apos;黄镇&apos;, &apos;党委书记&apos;, &apos;张涛&apos;, &apos;宣国&apos;, &apos;才&apos;, &apos;真能&apos;, &apos;一手遮天&apos;, &apos;吗&apos;], [&apos;江歌&apos;, &apos;事件&apos;, &apos;教会&apos;, &apos;孩子&apos;, &apos;善良&apos;, &apos;的&apos;, &apos;同时&apos;, &apos;更要&apos;, &apos;懂得&apos;, &apos;保护&apos;, &apos;自己&apos;], [&apos;绝味&apos;, &apos;鸭&apos;, &apos;脖&apos;, &apos;广告&apos;, &apos;开&apos;, &apos;黄腔&apos;, &apos;引&apos;, &apos;众怒&apos;, &apos; &apos;, &apos;双&apos;, &apos;11&apos;, &apos;这么&apos;, &apos;拼值&apos;, &apos;吗&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;垃圾车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;], [&apos;恶劣&apos;, &apos;至极&apos;, &apos;央视&apos;, &apos;都&apos;, &apos;曝光&apos;, &apos;了&apos;, &apos;南通&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;洒水车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢&apos;, &apos;大发&apos;, &apos;了&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;南通&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢脸&apos;], [&apos;粉丝&apos;, &apos;爆料&apos;, &apos;五洲&apos;, &apos;国际&apos;, &apos;无锡&apos;, &apos;项目&apos;, &apos;涉嫌&apos;, &apos;诈骗&apos;, &apos;非法&apos;, &apos;集资&apos;], [&apos;年内&apos;, &apos;约&apos;, &apos;10&apos;, &apos;起&apos;, &apos;锂电&apos;, &apos;重组&apos;, &apos;失败&apos;, &apos; &apos;, &apos;资本&apos;, &apos;对&apos;, &apos;高&apos;, &apos;估值&apos;, &apos;收购&apos;, &apos;说&apos;, &apos;不&apos;], [&apos;男子&apos;, &apos;梦想&apos;, &apos;一夜&apos;, &apos;暴富&apos;, &apos;持&apos;, &apos;水泥块&apos;, &apos;砸机&apos;], [&apos;北京&apos;, &apos;多家&apos;, &apos;法院&apos;, &apos;供暖&apos;, &apos;纠纷&apos;, &apos;案件&apos;, &apos;主体&apos;, &apos;为&apos;, &apos;供暖费&apos;, &apos;追缴&apos;, &apos;山海&apos;, &apos;网&apos;]] Word Embedding然后将其Embedding，就是用向量表示文本。在word2vec以前，大都使用one-hot加tf-idf来向量化文本 12 12 12 12 12 12 12 然后划分训练集和验证集，这里按照9:1划分，由于没有测试集，这里小道用验证集当做测试集来计算得分。这里可以直接调用sklearn的划分数据集的函数，或者自己写一个划分函数，既然写到这儿了，我就自己写一个，顺便测测和sklearn划分函数的时间差距。 1234567891011121314151617181920212223242526272829303132333435363738#整体思路是先把索引random，然后按比例向下取整选取。import numpy as npimport osimport mathimport randomimport timedef heng_split(filein, fileout_dir, train_persent): is_exsist = os.path.exists(fileout_dir) if not is_exsist: os.mkdir(fileout_dir) else: print(\"%s目录已经存在\"%(fileout_dir)) with open(fileout_dir+\"/train.csv\", 'w', encoding='utf-8') as fout_train: with open(fileout_dir+\"/test.csv\", 'w', encoding='utf-8') as fout_test: with open(filein, encoding='utf-8') as fin: content = fin.readlines() count_line = len(content) before = [k for k in range(count_line)] random.shuffle(before) content =[content[i] for i in before] train_count = int(math.floor(count_line * train_persent)) for index, i in enumerate(content): if index &lt;= train_count: fout_train.write(i) else: fout_test.write(i)def main(): filein = \"train.tsv\" # for train_persent in np.arange(0.6,0.95,0.05): for train_persent in [0.95]: fileout_dir = \"out\" + str(train_persent)[:4] heng_split(filein, fileout_dir, train_persent)if __name__ == \"__main__\": main() #这里是sklearn的划分方法from sklearn.model_selection import train_test_split if name == “main“: data = pd.read_csv(“G:/dataset/wine.csv”) #将样本分为x表示特征，y表示类别 x,y = data.ix[:,1:],data.ix[:,0] #测试集为30%，训练集为70% x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0) print(len(x_train)) #124 print(len(x_test)) 12","link":"/2019/10/23/文本分类/"},{"title":"聚类算法详解-从文本聚类谈起","text":"无监督的将相似句聚类，所以需要对聚类算法有比较深入的研究，单纯的调用sklearn无法满足工作需求，故对其进行实现。本文主要分三部分，第一部分描述对层次聚类的总结(因为这是工作中最终选取的算法)，第二部分描述kmeans算法(谈聚类怎么能不谈kmeans)，第三部分描述神经网络的聚类算法 算法详解层次聚类的步骤，终止条件分为两种，一种是聚到k类停止，一种是每两类之间的阈值大于某数。由于sklearn已经实现第一种方法，这里就不在赘述，主要分析一下第二种终止条件的实现及遇到的一些问题，全部代码请参考我的github。 算法原理层次聚类，顾名思义是将数据点分层进行聚类，主要有两种思路，一种是凝聚法，即算法开始时将每个节点设为一类，然后按照类间距离开始凝聚，知道凝聚满足终止条件。另一种是分裂法，即算法开始时所有数据点视为一类，然后开始分裂，直到分裂满足终止条件。 这里以凝聚法为例（分裂法原理一样，只是刚好相反），层次聚类涉及的主要问题在于类别之间的距离计算及聚类中心点的选择。 距离计算公式其他的距离计算公式在这里也不赘述，主流的距离度量方法可以参考sklearn的实现，sklearn的凝聚法实现了有6种距离度量，分别为“euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or ‘precomputed’。 由于我们是文本聚类，所以选择文本中最常用的余弦距离（选择余弦是有道理的，请自行google，或者等我填坑。。。）。 聚类中心点选择这里选择平均距离，因为经测试后，这种距离效果最好。 代码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215#层次聚类，两类之间使用平均距离作为类间距离from sklearn import preprocessingimport numpy as npimport heng_hierarchical_pre as h_preimport timeclass Hierarchical(object): \"\"\" 层次聚类 :param: min_dist -&gt;float32 聚类终止条件 :param: sub_node_id -&gt;list 存放聚完类的节点 :param: point_num -&gt;int 节点个数 :param: feature_num -&gt;int 节点特征数 \"\"\" def __init__(self, min_dist=0.63): self.min_dist = min_dist self.point_num = 0 self.feature_num = 0 self.distance_ori = None def binary_search1(self,nodes_pair_sort, x): ''' 二分查找 ''' left, right = 0, len(nodes_pair_sort)-1 if x&gt;=nodes_pair_sort[0]: return 0 if x&lt;=nodes_pair_sort[-1]: return -1 while left&lt;right: mid = int((left + right) / 2) if nodes_pair_sort[mid] &lt; x: right = mid if nodes_pair_sort[mid-1] &gt;= x: return mid-1 else: left = mid + 1 if nodes_pair_sort[mid+1] &lt;= x: return mid def binary_search2(self,nodes_pair_sort, x): ''' 二分查找 ''' left, right = 0, len(nodes_pair_sort)-1 if x&gt;=nodes_pair_sort[0][2]: return 0 if x&lt;=nodes_pair_sort[-1][2]: return -1 while left&lt;right: mid = int((left + right) / 2) if nodes_pair_sort[mid][2] &lt; x: right = mid if nodes_pair_sort[mid-1][2] &gt;= x: return mid else: left = mid + 1 if nodes_pair_sort[mid+1][2] &lt;= x: return mid+1 def fit(self, data): \"\"\" 训练模型，得到最终聚类结果 :param : data -&gt;np.array() 传入的训练数据 :param : distance_final -&gt;list 满足约束条件的节点对组成的列表 \"\"\" nodes = {} nodes_all_between_dist = {} data = np.array(data) self.point_num, self.feature_num = np.shape(data)[0], np.shape(data)[1] data = preprocessing.normalize(data,'l2') distance_big = np.zeros([self.point_num*2,self.point_num*2]) distance_ori = np.dot(data, data.T) self.distance_ori = distance_ori for i in range(self.point_num): distance_ori[i][i]=0 distance_half = np.triu(distance_ori) for i in range(self.point_num): nodes[i] = [i] nodes_all_between_dist[i] = distance_ori[i] min_dist = self.min_dist index_min_dist = np.where(distance_half &gt;= min_dist) nodes_pair = list()#存放全部满足条件节点对(left,right,dist) node_id_new = self.point_num#新节点id #满足条件节点对 for i in range(len(index_min_dist[0])): raw_distance, colume_distance = index_min_dist[0][i], index_min_dist[1][i] nodes_pair.append((raw_distance,colume_distance,distance_half[raw_distance][colume_distance])) nodes_before_del = set() nodes_pair_sort_1 = sorted(nodes_pair, key=lambda x:x[2], reverse=True) len_nodes_pair_sort = len(nodes_pair_sort_1) #每1000个放一个列表noedes_pair_sort = [[],[],...] print('排序前列表长度%d'%(len_nodes_pair_sort)) nodes_pair_sort = [] nodes_pair_range = np.arange(0, len_nodes_pair_sort, 16000) for index, value in enumerate(nodes_pair_range[:-1]): nodes_pair_sort.append(nodes_pair_sort_1[nodes_pair_range[index]:nodes_pair_range[index+1]]) nodes_pair_sort.append(nodes_pair_sort_1[nodes_pair_range[-1]:len_nodes_pair_sort]) count=0 distance_big[:self.point_num, :self.point_num] = distance_ori print('排序后列表长度%d'%(len(nodes_pair_sort))) while len(nodes_pair_sort)&gt;1: # start1 = time.time() # np.sort() # start2 = time.time() # print('排序耗时%f'%(start2-start1)) if nodes_pair_sort[0] == []: del nodes_pair_sort[0] #将每个组的最大的数加入nodes_pair_range_head中 nodes_pair_range_head = [] for value in nodes_pair_sort: # print(value) nodes_pair_range_head.append(value[0][2]) left, right = nodes_pair_sort[0][0][0], nodes_pair_sort[0][0][1] if left in nodes_before_del or right in nodes_before_del: del nodes_pair_sort[0][0] else: count+=1 nodes_before_del.add(left) nodes_before_del.add(right) # print('聚类的左右节点为%s,%s\\n'%(left,right)) # file_test.write('聚类的左右节点为%s,%s\\n'%(left,right)) nodes[node_id_new] = [] nodes[node_id_new] += nodes[left] nodes[node_id_new] += nodes[right] # print('新节点及其子节点为%s,%s'%(node_id_new,nodes[node_id_new])) start_insert = time.time() new_distance_1 = (distance_big[left]*len(nodes[left]) + distance_big[right]*len(nodes[right])) new_distance = new_distance_1/(len(nodes[left])+len(nodes[right])) # print(new_distance.shape) distance_big[:,node_id_new] = new_distance # print(distance_ori.shape) distance_big[node_id_new,:] = new_distance.reshape(1,len(new_distance)) end_insert = time.time() # print('计算节点耗时%f'%(end_insert - start_insert)) # print(distance_ori.shape) # print('节点有:%s'%' '.join([str(i) for i in nodes])) start_out = time.time() for node in nodes_before_del: distance_big[node,node_id_new] = 0 distance_big[node_id_new,node] = 0 nodes.pop(left) nodes.pop(right) node_new_dist = distance_big[:,node_id_new] count_insert = 0 start_getNewdict = time.time() for count_1,i in enumerate(node_new_dist): if i &gt;= min_dist: count_insert += 1 # print(nodes_pair_range_head) # print(len(nodes_pair_range_head)) nodes_pair_index = self.binary_search1(nodes_pair_range_head, i) # print(nodes_pair_index, i) index = self.binary_search2(nodes_pair_sort[nodes_pair_index], i) nodes_pair_sort[nodes_pair_index].insert(index,(node_id_new,count_1,i)) end_getNewdict = time.time() # print('查找插入索引耗时%f'%(end_getNewdict - start_getNewdict)) # print('插入次数%d'%count_insert) node_id_new += 1 end_out = time.time() # print('循环总耗时%f'%(end_out-start_out)) start_insert = time.time() labels = [nodes[i] for i in nodes] print('排序类别为%s'%len(nodes)) temp_labels = [] for index, value in enumerate(labels): if len(value)&gt;1: for j in value: temp_labels.append((j,index)) else: temp_labels.append((value[0],index)) labels = sorted(temp_labels,key=lambda x:x[0]) labels = [j for i,j in labels] end_insert = time.time() # print('排序节点耗时%f'%(end_insert - start_insert)) print('排序次数%d'%count) return labels def main(): start1 = time.time() data,_ = h_pre.main('udesk_test2.txt') print(data.shape) start2 = time.time() print('读取词向量耗时%.6s秒'%str(start2-start1)) print('*'*40) my1 = time.time() clust = Hierarchical(0.6) clust.fit(data) end1 = time.time() print('程序耗时%.6f秒'%float(end1-my1))if __name__ == '__main__': main()","link":"/2018/09/01/浅谈文本聚类/"}],"tags":[{"name":"nlp","slug":"nlp","link":"/tags/nlp/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"书单","slug":"书单","link":"/categories/书单/"},{"name":"电影观后感","slug":"电影观后感","link":"/categories/电影观后感/"},{"name":"深度学习","slug":"深度学习","link":"/categories/深度学习/"},{"name":"自然语言理解","slug":"自然语言理解","link":"/categories/自然语言理解/"},{"name":"旅行","slug":"旅行","link":"/categories/旅行/"},{"name":"机器学习","slug":"机器学习","link":"/categories/机器学习/"}]}