{"pages":[{"title":"旅行记","text":"","link":"/book/index - 副本 (10).html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (2).html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (3).html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (4).html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (5).html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (6).html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (7).html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (8).html"},{"title":"旅行记","text":"","link":"/book/index - 副本 (9).html"},{"title":"Pandas相关操作","text":"Pandas相关操作Pandas 是python的一个数据分析包，在做NLP任务时可以极大的提高我们的处理效率，所以需要一些入门的知识。小道会在这章里自己涉及到的全部pandas操作。文章是用MD写的，大家可以根据右侧的目录查看自己感兴趣的操作。 pandas读取csv文件pandas读取csv文件使用函数read_csv,可以将csv文件读取为DataFrame。这里用示例详细演示。这里写一个csv文件，用作测试 12345678910with open(\"data/test_pandas_read.csv\", 'w', encoding='utf-8') as fout: fout.write(\"姓名,性别,年龄\\n\") fout.write(\"张一,男,21\\n\") fout.write(\"张二,男,22\\n\") fout.write(\"张三,男,23\\n\") fout.write(\"张四,男,24\\n\") fout.write(\"张五,男,25\\n\") fout.write(\"张六,男,26\\n\") fout.write(\"张七,男,27\\n\") fout.write(\"张八,男,28\\n\") 123import pandas as pdtrain_data = pd.read_csv(\"data/test_pandas_read.csv\")print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 说明一下几个参数 sep:分隔符，默认逗号 header:指定行数作为列名，默认为第1行，如果没有header行就设置为None index_col:用作行索引的列编号 names:用于结果的列名列表 skiprows：忽略的行数 nrows：读取的行数 123#这里使用123作列索引，把第一行也作为数据读取，只读取前6行train_data_test = pd.read_csv(\"data/test_pandas_read.csv\", sep=\",\", header=None, names=['1','2','3'], nrows=6)print(train_data_test) 1234567 1 2 30 姓名 性别 年龄1 张一 男 212 张二 男 223 张三 男 234 张四 男 245 张五 男 25 pandas 的基本DataFrame操作DataFrame 是一种二维的数据结构，非常接近于电子表格或者类似 mysql 数据库的形式。它的竖行称之为 columns，横行称之为 index，也就是说可以通过 columns 和 index 来确定一个主句的位置。上文的文件读入之后会成为一个DataFrame对象，下面列举常用的几个方法： 1print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 DataFrame的创建 通过列表创建 123456789tmp = [1,2,3,4]print(\"create with default index and column\")print(pd.DataFrame(tmp))print(\"create with own index and column\")print(pd.DataFrame(tmp, columns=[\"name\"], index=[\"num1\",\"num2\",\"num3\",\"num4\"]))#二维列表创建tmp_dim2 = [[1,2,3],[2,3,4]]print(\"create use dimension 2 list\")# print(pd.DataFrame(tmp_dim2) 12345678910111213create with default index and column 00 11 22 33 4create with own index and column namenum1 1num2 2num3 3num4 4create use dimension 2 list 通过numpy数组创建 1234567import numpy as nptmp = np.array([[1,2,3],[1,2,3]])print(pd.DataFrame(tmp))np.random.randint(12)tmp = np.reshape(np.random.random(12),(3,4))print(\"numpy array use random\")print(pd.DataFrame(tmp)) 12345678 0 1 20 1 2 31 1 2 3numpy array use random 0 1 2 30 0.408672 0.161733 0.583770 0.0816801 0.501434 0.156215 0.422717 0.0795052 0.149095 0.683209 0.984084 0.927738 通过字典创建 1print(pd.DataFrame({'name':[\"小道1\",\"小道2\"],'age':[23,24]})) 123 name age0 小道1 231 小道2 24 DataFrame的常用方法查看数据 查看前几行：head方法，默认5行 查看后几行：tail方法，默认5行 查看索引:index, column 查看数据:values，loc, iloc 1print(train_data.head()) 123456 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 25 1print(train_data.tail(3)) 1234 姓名 性别 年龄5 张六 男 266 张七 男 277 张八 男 28 123print(\"index of row\\n\",train_data.index)print(\"index of column\\n\",train_data.columns)print(\"data values\\n\",train_data.values) 12345678910111213index of row RangeIndex(start=0, stop=8, step=1)index of column Index([&apos;姓名&apos;, &apos;性别&apos;, &apos;年龄&apos;], dtype=&apos;object&apos;)data values [[&apos;张一&apos; &apos;男&apos; 21] [&apos;张二&apos; &apos;男&apos; 22] [&apos;张三&apos; &apos;男&apos; 23] [&apos;张四&apos; &apos;男&apos; 24] [&apos;张五&apos; &apos;男&apos; 25] [&apos;张六&apos; &apos;男&apos; 26] [&apos;张七&apos; &apos;男&apos; 27] [&apos;张八&apos; &apos;男&apos; 28]] 查看行数据与列数据 123print(\"查看列数据\\n%s\"%train_data[\"姓名\"])print(\"\\n查看行数据\\n%s\"%train_data[0:1])#这里查看行时只能用连续索引的形式，如果是单索引（train_data[0]）会报错 1234567891011121314查看列数据0 张一1 张二2 张三3 张四4 张五5 张六6 张七7 张八Name: 姓名, dtype: object查看行数据 姓名 性别 年龄0 张一 男 21 可以使用loc函数通过标签查看及选取数据 1print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 也可以使用iloc函数通过位置查看和选取数据,这也是小道最常用的取数据方式 1234tmp = train_data.iloc[0:2,0:2]print(\"取前两行两列的数据\\n%s\"%tmp)tmp = train_data.iloc[1,2]print(\"\\n取第二行的第3列的数据\\n%s\"%tmp) 1234567取前两行两列的数据 姓名 性别0 张一 男1 张二 男取第二行的第3列的数据22 对数据的统计：describe函数 1234print(\"字符串信息统计\\n%s\"%train_data[\"姓名\"].describe())print(\"数值信息统计\\n%s\"%train_data[\"年龄\"].describe())print(\"count is \"%train_data[\"年龄\"].describe().count())print(\"max num is \"%train_data[\"年龄\"].describe().max()) 123456789101112131415161718字符串信息统计count 8unique 8top 张二freq 1Name: 姓名, dtype: object数值信息统计count 8.00000mean 24.50000std 2.44949min 21.0000025% 22.7500050% 24.5000075% 26.25000max 28.00000Name: 年龄, dtype: float64count is max num is 对列进行排序 1print(train_data.sort_index(axis=1,ascending=True)) 123456789 姓名 年龄 性别0 张一 21 男1 张二 22 男2 张三 23 男3 张四 24 男4 张五 25 男5 张六 26 男6 张七 27 男7 张八 28 男 1train_data[\"年龄\"].sort() 123456---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-65-7a17c7a44327&gt; in &lt;module&gt;----&gt; 1 train_data[&quot;年龄&quot;].sort() 123456~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in __getattr__(self, name) 5065 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5066 return self[name]-&gt; 5067 return object.__getattribute__(self, name) 5068 5069 def __setattr__(self, name, value): 1AttributeError: &apos;Series&apos; object has no attribute &apos;sort&apos; 缺失值的处理1在pandas中，缺失值处理也是常见问题。","link":"/book/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"外面的世界","text":"最愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光，就令荧火一般，也可以在黑暗里发一点光，不必等候炬火。——鲁迅《热风·随感录四十一》 小时候，村子好大，梦想着有一天可以从村东头走到村西头。 长大后，却发现我们的世界好小，外面的世界好大。 慢慢长大，也在慢慢遗忘。在这里记录下我一些我认为重要的东西。希望能在垂垂暮年之际在这里可以回忆一生。","link":"/about/index.html"},{"title":"文本分类","text":"相较于短文本，处理长文本时，需要更加精细的处理，这里针对不同长度的文本，分别做研究。 短文本分类数据集介绍在网上找了好久，最终决定使用DataFontain平台的互联网新闻情感分析竞赛数据集来作为这次短文本分类的训练用数据集。竞赛简介如下：本赛题目标为在庞大的数据集中精准的区分文本的情感极性，情感分为正中负三类。面对浩如烟海的新闻信息，精确识别蕴藏在其中的情感倾向，对舆情有效监控、预警及疏导，对舆情生态系统的良性发展有着重要的意义。 数据集示例如下： 123import pandas as pdtrain_data = pd.read_csv(\"data/Train_DataSet.csv\")train_data.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } id title content 0 7a3dd79f90ee419da87190cff60f7a86 问责领导(上黄镇党委书记张涛，宣国才真能一手遮天吗？) 这几天看了有人举报施某某的贴子，经与举报人联系证实，是宣某当天中午请举报人和枪手喝酒后，晚上... 1 7640a5589bc7486ca199eeeb38af79dd 江歌事件:教会孩子，善良的同时更要懂得保护自己! 过去一年的江歌悲剧，这几日再次刷屏：住在东京都中野区的中国女留学生江歌，收留了被前男友陈世锋... 2 8c5bda93e4ba401f90a0faa5b28fe57f 绝味鸭脖广告\"开黄腔\"引众怒 \"双11\"这么拼值吗? “双11”1600亿的销售额让中国乃至全世界感到震惊，为此，不仅有不惜欠债百万的“剁手党”，... 3 1aa777fed31a4b8a9d866f05b5477557 央视曝光!如东一医药企业将槽罐车改成垃圾车，夜间偷排高浓度废水 2016年以来，如东高新区一家叫做拜瑞生物医药的企业竟然将装废水的槽罐车伪装成洒水车，常年在... 4 6c67ac55360340258e157f3710ebae6c 恶劣至极，央视都曝光了!南通如东一医药企业将槽罐车改成洒水车，夜间偷排高浓度废水...丢大发了! 新闻资讯•民生热点•城市生活•南通人都在看浏览更多资讯请关注微信公众号:南通传媒网昨晚央视一... 在本章中，我们只使用新闻标题作为训练数据，来判断新闻的类别，在下章的长文本分类时在使用新闻的content，最终结合使用所有的数据得到最终的分数。目前初赛快结束了，分数最高的是0.8231。在本文写完之后看下最终的分数能否达到近似的水平，由于我没有测试集的label，所以使用训练集划分的分数作为最终的分数，差别应该不会很大。那我们现在就开始系统的文本分类探究吧！ 数据预处理数据预处理是做NLP必不可少的流程之一。由于是NLP的第一篇文章，所以这里对预处理做详细的说明，比较几种预处理的结果。常用的预处理流程如下： 根据数据集的情况，有时需要去掉数字或者链接或者符号之类的 ，或者对数字做归一化处理等 分词 去停用词一般而言，这3步就是对文本的预处理流程了、现在开始实战。 12train_label = pd.read_csv(\"data/Train_DataSet_Label.csv\", header=0)train_label.head(5) id label 0 7a3dd79f90ee419da87190cff60f7a86 2 1 7640a5589bc7486ca199eeeb38af79dd 1 2 8c5bda93e4ba401f90a0faa5b28fe57f 2 3 1aa777fed31a4b8a9d866f05b5477557 2 4 6c67ac55360340258e157f3710ebae6c 2 先把文本和label对应一下。 12345678910id_label = {i[0]:i[1] for i in train_label.values}id_train = {i[0]:i[1] for i in train_data.values}# print(id_label)text_label = [(id_train[i], id_label[i]) for i in id_label if i in id_train]print(\"\\n\".join([str(i) for i in text_label[:5]]))print(\"the length of text_label\",len(text_label))print(\"the length of train_data\",len(train_data.values))#将text和label分别作为listtext_all = [text for text,_ in text_label]label_all = [label for _,label in text_label] 1234567(&apos;问责领导(上黄镇党委书记张涛，宣国才真能一手遮天吗？)&apos;, 2)(&apos;江歌事件:教会孩子，善良的同时更要懂得保护自己!&apos;, 1)(&apos;绝味鸭脖广告&quot;开黄腔&quot;引众怒 &quot;双11&quot;这么拼值吗?&apos;, 2)(&apos;央视曝光!如东一医药企业将槽罐车改成垃圾车，夜间偷排高浓度废水&apos;, 2)(&apos;恶劣至极，央视都曝光了!南通如东一医药企业将槽罐车改成洒水车，夜间偷排高浓度废水...丢大发了!&apos;, 2)the length of text_label 7340the length of train_data 7345 可以看出有5条是id没对应起来，这5条咱就不要了，对大局没影响。 接下来需要对文本做预处理。这里我们做个对比试验，一种是经过预处理的，另一种是未预处理，查看预处理对各种算法的影响。这里需要特别指出的是：现在的深度模型基本不需要对文本做预处理了，很多情况下基于字的特征比词的特征具有更高的准确率，后续小道会针对这个问题作详细的对比实验与说明。但预处理作为NLP的重要基础之一，还是需要了解一下滴本章预处理流程为：分词，去标点，去停用词（也可以将标点写在停用词表内一步到位，这样分开是因为有时候标点和停用词的去除处理不同） 分词（这里使用python最常用的分词包jieba）123import jiebatext_data = [jieba.lcut(str(sentence)) for sentence in text_all]print(text_data[-10:]) 1[[&apos;帮&apos;, &apos;女郎&apos;, &apos;在&apos;, &apos;行动&apos;], [&apos;稻城&apos;, &apos;亚丁&apos;, &apos;+&apos;, &apos;318&apos;, &apos;川藏线&apos;, &apos;+&apos;, &apos;拉萨&apos;, &apos;+&apos;, &apos;青藏线&apos;, &apos;，&apos;, &apos;你&apos;, &apos;心动&apos;, &apos;了&apos;, &apos;吗&apos;, &apos;？&apos;], [&apos;大良&apos;, &apos;、&apos;, &apos;龙江&apos;, &apos;、&apos;, &apos;陈村&apos;, &apos;4&apos;, &apos;所&apos;, &apos;学校&apos;, &apos;9&apos;, &apos;月&apos;, &apos;开学&apos;, &apos;!&apos;, &apos;顺德&apos;, &apos;未来&apos;, &apos;两年&apos;, &apos;力&apos;, &apos;...&apos;], [&apos;最&apos;, &apos;容易&apos;, &apos;受到&apos;, &apos;甲醛&apos;, &apos;侵害&apos;, &apos;的&apos;, &apos;五类&apos;, &apos;人&apos;, &apos;，&apos;, &apos;有&apos;, &apos;你&apos;, &apos;吗&apos;, &apos;？&apos;, &apos;_&apos;, &apos;污染&apos;], [&apos;2019&apos;, &apos;年&apos;, &apos;1&apos;, &apos;月&apos;, &apos;事故&apos;, &apos;伤亡&apos;, &apos;月&apos;, &apos;报&apos;], [&apos;珊瑚&apos;, &apos;裸尾鼠&apos;, &apos;：&apos;, &apos;首个&apos;, &apos;因&apos;, &apos;全球&apos;, &apos;气候&apos;, &apos;变暖&apos;, &apos;灭绝&apos;, &apos;的&apos;, &apos;哺乳动物&apos;], [&apos;独居&apos;, &apos;老人&apos;, &apos;做饭&apos;, &apos;忘关&apos;, &apos;火&apos;, &apos; &apos;, &apos;南通&apos;, &apos;志愿者&apos;, &apos;及时发现&apos;, &apos;转危为安&apos;], [&apos;被&apos;, &apos;生意&apos;, &apos;上&apos;, &apos;的&apos;, &apos;人&apos;, &apos;给&apos;, &apos;利用&apos;, &apos;合同诈骗&apos;, &apos;，&apos;, &apos;诈骗&apos;, &apos;三十万&apos;, &apos;够判&apos;, &apos;多少&apos;, &apos;年&apos;, &apos;-&apos;, &apos;-&apos;, &apos;在&apos;, &apos;..._&apos;, &apos;律师&apos;, &apos;365&apos;], [&apos;奎山&apos;, &apos;汽贸&apos;, &apos;城&apos;, &apos;去年&apos;, &apos;那场&apos;, &apos;火灾&apos;, &apos;，&apos;, &apos;调查&apos;, &apos;情况&apos;, &apos;报告&apos;, &apos;出来&apos;, &apos;了&apos;, &apos;！&apos;], [&apos;曝光&apos;, &apos;台&apos;, &apos;•&apos;, &apos;调查&apos;, &apos;｜&apos;, &apos;市场&apos;, &apos;消防通道&apos;, &apos;被&apos;, &apos;长期&apos;, &apos;霸占&apos;, &apos;？&apos;, &apos;事情&apos;, &apos;并非&apos;, &apos;想象&apos;, &apos;的&apos;, &apos;那样&apos;]] 去除标点符号可以使用string自带的punctuation获取英文标点和zhon包的punctuation获取中文标点 12345678910111213141516171819202122import stringimport zhon.hanzi as hanprint(\"English puctuation is :\",string.punctuation)print(\"Chinese puctuation is :\",han.punctuation)#需要查询操作，所以这里选用查找时间复杂度O(1)的集合punc = set()for i in string.punctuation: punc.add(i)for i in han.punctuation: punc.add(i)#这里举个例子sentence = \"fjdk sal fgjd,.,/,/,/.,.[]\"sentence_after = ''.join(word for word in sentence if word not in punc)print(\"sentence remove punctuation is :\",sentence_after)text_remove_punc = []for sentence in text_data: tmp = [] for word in sentence: if word not in punc: tmp.append(word) text_remove_punc.append(tmp)print(text_remove_punc[:10]) 1234English puctuation is : !&quot;#$%&amp;&apos;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~Chinese puctuation is : ＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､ 、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·！？｡。sentence remove punctuation is : fjdk sal fgjd[[&apos;问责&apos;, &apos;领导&apos;, &apos;上&apos;, &apos;黄镇&apos;, &apos;党委书记&apos;, &apos;张涛&apos;, &apos;宣国&apos;, &apos;才&apos;, &apos;真能&apos;, &apos;一手遮天&apos;, &apos;吗&apos;], [&apos;江歌&apos;, &apos;事件&apos;, &apos;教会&apos;, &apos;孩子&apos;, &apos;善良&apos;, &apos;的&apos;, &apos;同时&apos;, &apos;更要&apos;, &apos;懂得&apos;, &apos;保护&apos;, &apos;自己&apos;], [&apos;绝味&apos;, &apos;鸭&apos;, &apos;脖&apos;, &apos;广告&apos;, &apos;开&apos;, &apos;黄腔&apos;, &apos;引&apos;, &apos;众怒&apos;, &apos; &apos;, &apos;双&apos;, &apos;11&apos;, &apos;这么&apos;, &apos;拼值&apos;, &apos;吗&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;垃圾车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;], [&apos;恶劣&apos;, &apos;至极&apos;, &apos;央视&apos;, &apos;都&apos;, &apos;曝光&apos;, &apos;了&apos;, &apos;南通&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;洒水车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢&apos;, &apos;大发&apos;, &apos;了&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;南通&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢脸&apos;], [&apos;粉丝&apos;, &apos;爆料&apos;, &apos;五洲&apos;, &apos;国际&apos;, &apos;无锡&apos;, &apos;项目&apos;, &apos;涉嫌&apos;, &apos;诈骗&apos;, &apos;非法&apos;, &apos;集资&apos;], [&apos;年内&apos;, &apos;约&apos;, &apos;10&apos;, &apos;起&apos;, &apos;锂电&apos;, &apos;重组&apos;, &apos;失败&apos;, &apos; &apos;, &apos;资本&apos;, &apos;对&apos;, &apos;高&apos;, &apos;估值&apos;, &apos;收购&apos;, &apos;说&apos;, &apos;不&apos;], [&apos;男子&apos;, &apos;梦想&apos;, &apos;一夜&apos;, &apos;暴富&apos;, &apos;持&apos;, &apos;水泥块&apos;, &apos;砸机&apos;], [&apos;北京&apos;, &apos;多家&apos;, &apos;法院&apos;, &apos;供暖&apos;, &apos;纠纷&apos;, &apos;案件&apos;, &apos;主体&apos;, &apos;为&apos;, &apos;供暖费&apos;, &apos;追缴&apos;, &apos;山海&apos;, &apos;网&apos;]] 去除停用词这里提供我自己常用的停用词表，大家可以根据不同的需求自己设定停用词表 123456789101112with open(\"data/stopwords\", encoding='utf-8') as fin: stopwords = set() for i in fin: stopwords.add(i.strip())text_after = []for sentence in text_remove_punc: tmp = [] for word in sentence: if word not in stopwords: tmp.append(word) text_after.append(tmp)print(text_after[:10]) 1[[&apos;问责&apos;, &apos;领导&apos;, &apos;上&apos;, &apos;黄镇&apos;, &apos;党委书记&apos;, &apos;张涛&apos;, &apos;宣国&apos;, &apos;才&apos;, &apos;真能&apos;, &apos;一手遮天&apos;, &apos;吗&apos;], [&apos;江歌&apos;, &apos;事件&apos;, &apos;教会&apos;, &apos;孩子&apos;, &apos;善良&apos;, &apos;的&apos;, &apos;同时&apos;, &apos;更要&apos;, &apos;懂得&apos;, &apos;保护&apos;, &apos;自己&apos;], [&apos;绝味&apos;, &apos;鸭&apos;, &apos;脖&apos;, &apos;广告&apos;, &apos;开&apos;, &apos;黄腔&apos;, &apos;引&apos;, &apos;众怒&apos;, &apos; &apos;, &apos;双&apos;, &apos;11&apos;, &apos;这么&apos;, &apos;拼值&apos;, &apos;吗&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;垃圾车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;], [&apos;恶劣&apos;, &apos;至极&apos;, &apos;央视&apos;, &apos;都&apos;, &apos;曝光&apos;, &apos;了&apos;, &apos;南通&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;洒水车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢&apos;, &apos;大发&apos;, &apos;了&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;南通&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢脸&apos;], [&apos;粉丝&apos;, &apos;爆料&apos;, &apos;五洲&apos;, &apos;国际&apos;, &apos;无锡&apos;, &apos;项目&apos;, &apos;涉嫌&apos;, &apos;诈骗&apos;, &apos;非法&apos;, &apos;集资&apos;], [&apos;年内&apos;, &apos;约&apos;, &apos;10&apos;, &apos;起&apos;, &apos;锂电&apos;, &apos;重组&apos;, &apos;失败&apos;, &apos; &apos;, &apos;资本&apos;, &apos;对&apos;, &apos;高&apos;, &apos;估值&apos;, &apos;收购&apos;, &apos;说&apos;, &apos;不&apos;], [&apos;男子&apos;, &apos;梦想&apos;, &apos;一夜&apos;, &apos;暴富&apos;, &apos;持&apos;, &apos;水泥块&apos;, &apos;砸机&apos;], [&apos;北京&apos;, &apos;多家&apos;, &apos;法院&apos;, &apos;供暖&apos;, &apos;纠纷&apos;, &apos;案件&apos;, &apos;主体&apos;, &apos;为&apos;, &apos;供暖费&apos;, &apos;追缴&apos;, &apos;山海&apos;, &apos;网&apos;]] Word Embedding然后将其Embedding，就是用向量表示文本。在word2vec以前，大都使用one-hot加tf-idf来向量化文本 12 12 12 12 12 12 12 然后划分训练集和验证集，这里按照9:1划分，由于没有测试集，这里小道用验证集当做测试集来计算得分。这里可以直接调用sklearn的划分数据集的函数，或者自己写一个划分函数，既然写到这儿了，我就自己写一个，顺便测测和sklearn划分函数的时间差距。 1234567891011121314151617181920212223242526272829303132333435363738#整体思路是先把索引random，然后按比例向下取整选取。import numpy as npimport osimport mathimport randomimport timedef heng_split(filein, fileout_dir, train_persent): is_exsist = os.path.exists(fileout_dir) if not is_exsist: os.mkdir(fileout_dir) else: print(\"%s目录已经存在\"%(fileout_dir)) with open(fileout_dir+\"/train.csv\", 'w', encoding='utf-8') as fout_train: with open(fileout_dir+\"/test.csv\", 'w', encoding='utf-8') as fout_test: with open(filein, encoding='utf-8') as fin: content = fin.readlines() count_line = len(content) before = [k for k in range(count_line)] random.shuffle(before) content =[content[i] for i in before] train_count = int(math.floor(count_line * train_persent)) for index, i in enumerate(content): if index &lt;= train_count: fout_train.write(i) else: fout_test.write(i)def main(): filein = \"train.tsv\" # for train_persent in np.arange(0.6,0.95,0.05): for train_persent in [0.95]: fileout_dir = \"out\" + str(train_persent)[:4] heng_split(filein, fileout_dir, train_persent)if __name__ == \"__main__\": main() #这里是sklearn的划分方法from sklearn.model_selection import train_test_split if name == “main“: data = pd.read_csv(“G:/dataset/wine.csv”) #将样本分为x表示特征，y表示类别 x,y = data.ix[:,1:],data.ix[:,0] #测试集为30%，训练集为70% x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0) print(len(x_train)) #124 print(len(x_test)) 12","link":"/book/文本分类.html"}],"posts":[{"title":"Pandas相关操作","text":"Pandas 是python的一个数据分析包，在做NLP任务时可以极大的提高我们的处理效率，所以需要一些入门的知识。小道会在这章里自己涉及到的全部pandas操作。文章是用MD写的，大家可以根据右侧的目录查看自己感兴趣的操作。 pandas读取csv文件pandas读取csv文件使用函数read_csv,可以将csv文件读取为DataFrame。这里用示例详细演示。这里写一个csv文件，用作测试 12345678910with open(\"data/test_pandas_read.csv\", 'w', encoding='utf-8') as fout: fout.write(\"姓名,性别,年龄\\n\") fout.write(\"张一,男,21\\n\") fout.write(\"张二,男,22\\n\") fout.write(\"张三,男,23\\n\") fout.write(\"张四,男,24\\n\") fout.write(\"张五,男,25\\n\") fout.write(\"张六,男,26\\n\") fout.write(\"张七,男,27\\n\") fout.write(\"张八,男,28\\n\") 123import pandas as pdtrain_data = pd.read_csv(\"data/test_pandas_read.csv\")print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 说明一下几个参数 sep:分隔符，默认逗号 header:指定行数作为列名，默认为第1行，如果没有header行就设置为None index_col:用作行索引的列编号 names:用于结果的列名列表 skiprows：忽略的行数 nrows：读取的行数 123#这里使用123作列索引，把第一行也作为数据读取，只读取前6行train_data_test = pd.read_csv(\"data/test_pandas_read.csv\", sep=\",\", header=None, names=['1','2','3'], nrows=6)print(train_data_test) 1234567 1 2 30 姓名 性别 年龄1 张一 男 212 张二 男 223 张三 男 234 张四 男 245 张五 男 25 pandas 的基本DataFrame操作DataFrame 是一种二维的数据结构，非常接近于电子表格或者类似 mysql 数据库的形式。它的竖行称之为 columns，横行称之为 index，也就是说可以通过 columns 和 index 来确定一个主句的位置。上文的文件读入之后会成为一个DataFrame对象，下面列举常用的几个方法： 1print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 DataFrame的创建 通过列表创建 123456789tmp = [1,2,3,4]print(\"create with default index and column\")print(pd.DataFrame(tmp))print(\"create with own index and column\")print(pd.DataFrame(tmp, columns=[\"name\"], index=[\"num1\",\"num2\",\"num3\",\"num4\"]))#二维列表创建tmp_dim2 = [[1,2,3],[2,3,4]]print(\"create use dimension 2 list\")# print(pd.DataFrame(tmp_dim2) 12345678910111213create with default index and column 00 11 22 33 4create with own index and column namenum1 1num2 2num3 3num4 4create use dimension 2 list 通过numpy数组创建 1234567import numpy as nptmp = np.array([[1,2,3],[1,2,3]])print(pd.DataFrame(tmp))np.random.randint(12)tmp = np.reshape(np.random.random(12),(3,4))print(\"numpy array use random\")print(pd.DataFrame(tmp)) 12345678 0 1 20 1 2 31 1 2 3numpy array use random 0 1 2 30 0.408672 0.161733 0.583770 0.0816801 0.501434 0.156215 0.422717 0.0795052 0.149095 0.683209 0.984084 0.927738 通过字典创建 1print(pd.DataFrame({'name':[\"小道1\",\"小道2\"],'age':[23,24]})) 123 name age0 小道1 231 小道2 24 DataFrame的常用方法查看数据 查看前几行：head方法，默认5行 查看后几行：tail方法，默认5行 查看索引:index, column 查看数据:values，loc, iloc 1print(train_data.head()) 123456 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 25 1print(train_data.tail(3)) 1234 姓名 性别 年龄5 张六 男 266 张七 男 277 张八 男 28 123print(\"index of row\\n\",train_data.index)print(\"index of column\\n\",train_data.columns)print(\"data values\\n\",train_data.values) 12345678910111213index of row RangeIndex(start=0, stop=8, step=1)index of column Index([&apos;姓名&apos;, &apos;性别&apos;, &apos;年龄&apos;], dtype=&apos;object&apos;)data values [[&apos;张一&apos; &apos;男&apos; 21] [&apos;张二&apos; &apos;男&apos; 22] [&apos;张三&apos; &apos;男&apos; 23] [&apos;张四&apos; &apos;男&apos; 24] [&apos;张五&apos; &apos;男&apos; 25] [&apos;张六&apos; &apos;男&apos; 26] [&apos;张七&apos; &apos;男&apos; 27] [&apos;张八&apos; &apos;男&apos; 28]] 查看行数据与列数据 123print(\"查看列数据\\n%s\"%train_data[\"姓名\"])print(\"\\n查看行数据\\n%s\"%train_data[0:1])#这里查看行时只能用连续索引的形式，如果是单索引（train_data[0]）会报错 1234567891011121314查看列数据0 张一1 张二2 张三3 张四4 张五5 张六6 张七7 张八Name: 姓名, dtype: object查看行数据 姓名 性别 年龄0 张一 男 21 可以使用loc函数通过标签查看及选取数据 1print(train_data) 123456789 姓名 性别 年龄0 张一 男 211 张二 男 222 张三 男 233 张四 男 244 张五 男 255 张六 男 266 张七 男 277 张八 男 28 也可以使用iloc函数通过位置查看和选取数据,这也是小道最常用的取数据方式 1234tmp = train_data.iloc[0:2,0:2]print(\"取前两行两列的数据\\n%s\"%tmp)tmp = train_data.iloc[1,2]print(\"\\n取第二行的第3列的数据\\n%s\"%tmp) 1234567取前两行两列的数据 姓名 性别0 张一 男1 张二 男取第二行的第3列的数据22 对数据的统计：describe函数 1234print(\"字符串信息统计\\n%s\"%train_data[\"姓名\"].describe())print(\"数值信息统计\\n%s\"%train_data[\"年龄\"].describe())print(\"count is \"%train_data[\"年龄\"].describe().count())print(\"max num is \"%train_data[\"年龄\"].describe().max()) 123456789101112131415161718字符串信息统计count 8unique 8top 张二freq 1Name: 姓名, dtype: object数值信息统计count 8.00000mean 24.50000std 2.44949min 21.0000025% 22.7500050% 24.5000075% 26.25000max 28.00000Name: 年龄, dtype: float64count is max num is 对列进行排序 1print(train_data.sort_index(axis=1,ascending=True)) 123456789 姓名 年龄 性别0 张一 21 男1 张二 22 男2 张三 23 男3 张四 24 男4 张五 25 男5 张六 26 男6 张七 27 男7 张八 28 男 1train_data[\"年龄\"].sort() 123456---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-65-7a17c7a44327&gt; in &lt;module&gt;----&gt; 1 train_data[&quot;年龄&quot;].sort() 123456~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in __getattr__(self, name) 5065 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5066 return self[name]-&gt; 5067 return object.__getattribute__(self, name) 5068 5069 def __setattr__(self, name, value): 1AttributeError: &apos;Series&apos; object has no attribute &apos;sort&apos; 缺失值的处理1在pandas中，缺失值处理也是常见问题。","link":"/2018/10/21/Pandas基础/"},{"title":"最大似然-从朴素贝叶斯谈起","text":"","link":"/2018/10/09/最大似然-从朴素贝叶斯谈起/"},{"title":"浅谈文本表示","text":"文本在计算机中进行训练，首先得表示成一串向量，如何Embedding文本，是NLP中一个重要的研究方向。这里理一下目前的进展","link":"/2019/09/01/浅谈文本表示/"},{"title":"语言模型-从闲聊识别谈起","text":"背景:本文由下面一轮对话引出： 你：“过大嘎达发的撒旦法而我却人是” 机器人：“您的输入语句不通，语文是数学老师教的吧！” 那么机器人到底是怎么判断出一句话语句是否通顺呢？在客服场景中又有什么实际应用呢？ 方法:客服场景中有时需要判断用户的问句是否为乱输的无意义的，返回“请认真聊天，不要乱输”比返回万能回复会显得更加的智能，也可以用在智能质检上，检测答案或者问句的编写是否合理。 我们可以采用语言模型来判定句子的流畅度，并判断是否成句。 语言模型：通俗来讲，语言模型就是用来计算一组字成为句子的概率的模型，也就是判断一句话是否随机的概率。 另一个解释是：语言模型是假设一门语言所有可能的句子服从一个概率分布，每个句子出现的概率加起来是1，那么语言模型的任务就是预测每个句子在语言中出现的概率，对于语言中常见的句子，一个好的语言模型应该得到相对高的概率，对不合语法的句子，计算出的概率则趋近于零。 给定词语序列：$ S = W_1,W_2, …, W_k $，判断它成为句子的概率可表示为：$ P(S) = P(W_1,W_2,…,W_k) $ 我们使用$ P(w_3|w_1,w_2)$, 表示词$ w_1,w_2$后面是词$ w_3$的概率，现在我们需要计算一整句话的概率，例如“我,爱,北京,天安门”,需要计算“我爱北京天安门“同时出现 的概率，即$ P(我爱北京天安门) = P(我)P(爱|我)P(北京|我爱)P(天安门|我爱北京)$，计算这个概率的模型就叫做语言模型(Language Model)，随后即可用概率的链式法则来评估这一组随机变量的联合概率。一般的：$P(S) = P(w_1,w_2,…,w_k) = \\prod_{i=1}^k P(w_i|w_1,w_2,…,w_{k-1})$ 可这种方法会导致： 参数空间过大, 试想按上面方式计算$P(w_5 |w_1 ,w_2 ,w_3 ,w_4 )$，这里$w_i$都有一个词典大小取值的可能，记作|V|，则该模型的参数个数是$|V|^5$，而且这还不包含$P(w_4 | w_1, w_2, w_3)$的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法实用.(更为通俗的理解：还是以”我爱北京天安门“为例，$P(爱|我)$ 需要计算) 数据非常稀疏，导致最大似然概率接近0.(为什么最大似然接近0，本人在另一篇中对此做了解释，链接:) 统计语言模型在统计语言模型中为了减少参数数量，基于马尔可夫假设，采用2-gram模型，即我们可以认为下一个词的出现仅依赖于他前面的一个词，因此二元模型简化为： $P(S) = P(w_1)\\prod_{i=2}^kP(w_i|w_{i-1})$ 在此基础上进行拓展可演化为$n-gram$模型，假定文本中的每个词$w_i$只和其前面$n-1$个词相关，这时$P(w_i|w_1,w_2,…w_{i-1})=P(w_i|w_{i-n+1},w_{i-n+2},…w_{i-1})$ 这称为$n-gram$模型，其中$n$表示当前单词依赖它前面单词的个数，通常$n$取2、3、4,在本任务中，我们对$n=2，3$的情况做了实验，且对分词与未分词的情况分别做了实验。最终在$2-gram$且不分词的情况下效果达到最优。究其原因，可能是在本任务中，若是用户输的乱序文本则该句中基本每两个字之间都不成词。 $n-gram$ 模型的参数一般采用最大似然估计方法： $P(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)}{count(w_{i-1]})}$ 其中，count(x)表示词x在语料中出现的次数。 例子以真实句子为例对模型做一般说明。 例如，计算“今天能不能发货?”和”发货能今天不能”这两句话的ngram值。 以词级别为例，分词之后的句子为: 今天\\能\\不能\\发货 与发货\\能\\今天\\不能? 在本问题中，我选取人工客服聊天记录共100M作为训练预料，用来统计$n-gram$词频)各词的频率如下表所示: 词 词频 2-gram词 词频 今天 447 今天能 23 能 844 能不能 122 不能 481 不能发货 3 发货 501 发货? 20 ？ 3646 这里还需对句子加入起始符号与终止符号(我加的是start与end，不加的话会损失起始与终止的信息)，于是在此基础上加入$count(start今天)$与$count(?end)$ 于是2-gram分值:$P(今天能不能发货？)=P(start)P(今天|start)P(能|今天)P(不能|能)P(发货|不能)P(？|发货)P(end|？)$ 根据条件概率$P(A|B)=P(AB)/P(B)$,所以 $P(今天能不能发货？)=P(start今天)P(今天能)P(能不能)P(不能发货)(发货？)/（P(今天)P(能)P(不能)*P(发货)）$= 而在这个问题中，取得该词的概率可以近似为该词词频数/总词频数，即$P(今天)=count(今天)/总词频数=447/6066\\approx0.07$ 以此类推， 这样我们可以得到两个句子各自成句的概率： $P(今天能不能发货？)=0.000031$ $P(发货能今天不能？)=0.0000000053$ 故而“今天能不能发货”比“发货能今天不能”更加流畅。 进行预测概率时，除了统计语言模型外，目前使用较多的还有神经语言模型，这里就不详细展开了，大家感兴趣的话可以自己查查。 参考：https://blog.csdn.net/a635661820/article/details/43906731 https://zhuanlan.zhihu.com/machinelearningpku 《数学之美》第三章 统计语言模型 https://juejin.im/post/598c1941f265da3e190da56b","link":"/2019/08/20/语言模型-从闲聊识别谈起/"},{"title":"文本分类","text":"相较于短文本，处理长文本时，需要更加精细的处理，这里针对不同长度的文本，分别做研究。 短文本分类数据集介绍在网上找了好久，最终决定使用DataFontain平台的互联网新闻情感分析竞赛数据集来作为这次短文本分类的训练用数据集。竞赛简介如下：本赛题目标为在庞大的数据集中精准的区分文本的情感极性，情感分为正中负三类。面对浩如烟海的新闻信息，精确识别蕴藏在其中的情感倾向，对舆情有效监控、预警及疏导，对舆情生态系统的良性发展有着重要的意义。 数据集示例如下： 123import pandas as pdtrain_data = pd.read_csv(\"data/Train_DataSet.csv\")train_data.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } id title content 0 7a3dd79f90ee419da87190cff60f7a86 问责领导(上黄镇党委书记张涛，宣国才真能一手遮天吗？) 这几天看了有人举报施某某的贴子，经与举报人联系证实，是宣某当天中午请举报人和枪手喝酒后，晚上... 1 7640a5589bc7486ca199eeeb38af79dd 江歌事件:教会孩子，善良的同时更要懂得保护自己! 过去一年的江歌悲剧，这几日再次刷屏：住在东京都中野区的中国女留学生江歌，收留了被前男友陈世锋... 2 8c5bda93e4ba401f90a0faa5b28fe57f 绝味鸭脖广告\"开黄腔\"引众怒 \"双11\"这么拼值吗? “双11”1600亿的销售额让中国乃至全世界感到震惊，为此，不仅有不惜欠债百万的“剁手党”，... 3 1aa777fed31a4b8a9d866f05b5477557 央视曝光!如东一医药企业将槽罐车改成垃圾车，夜间偷排高浓度废水 2016年以来，如东高新区一家叫做拜瑞生物医药的企业竟然将装废水的槽罐车伪装成洒水车，常年在... 4 6c67ac55360340258e157f3710ebae6c 恶劣至极，央视都曝光了!南通如东一医药企业将槽罐车改成洒水车，夜间偷排高浓度废水...丢大发了! 新闻资讯•民生热点•城市生活•南通人都在看浏览更多资讯请关注微信公众号:南通传媒网昨晚央视一... 在本章中，我们只使用新闻标题作为训练数据，来判断新闻的类别，在下章的长文本分类时在使用新闻的content，最终结合使用所有的数据得到最终的分数。目前初赛快结束了，分数最高的是0.8231。在本文写完之后看下最终的分数能否达到近似的水平，由于我没有测试集的label，所以使用训练集划分的分数作为最终的分数，差别应该不会很大。那我们现在就开始系统的文本分类探究吧！ 数据预处理数据预处理是做NLP必不可少的流程之一。由于是NLP的第一篇文章，所以这里对预处理做详细的说明，比较几种预处理的结果。常用的预处理流程如下： 根据数据集的情况，有时需要去掉数字或者链接或者符号之类的 ，或者对数字做归一化处理等 分词 去停用词一般而言，这3步就是对文本的预处理流程了、现在开始实战。 12train_label = pd.read_csv(\"data/Train_DataSet_Label.csv\", header=0)train_label.head(5) id label 0 7a3dd79f90ee419da87190cff60f7a86 2 1 7640a5589bc7486ca199eeeb38af79dd 1 2 8c5bda93e4ba401f90a0faa5b28fe57f 2 3 1aa777fed31a4b8a9d866f05b5477557 2 4 6c67ac55360340258e157f3710ebae6c 2 先把文本和label对应一下。 12345678910id_label = {i[0]:i[1] for i in train_label.values}id_train = {i[0]:i[1] for i in train_data.values}# print(id_label)text_label = [(id_train[i], id_label[i]) for i in id_label if i in id_train]print(\"\\n\".join([str(i) for i in text_label[:5]]))print(\"the length of text_label\",len(text_label))print(\"the length of train_data\",len(train_data.values))#将text和label分别作为listtext_all = [text for text,_ in text_label]label_all = [label for _,label in text_label] 1234567(&apos;问责领导(上黄镇党委书记张涛，宣国才真能一手遮天吗？)&apos;, 2)(&apos;江歌事件:教会孩子，善良的同时更要懂得保护自己!&apos;, 1)(&apos;绝味鸭脖广告&quot;开黄腔&quot;引众怒 &quot;双11&quot;这么拼值吗?&apos;, 2)(&apos;央视曝光!如东一医药企业将槽罐车改成垃圾车，夜间偷排高浓度废水&apos;, 2)(&apos;恶劣至极，央视都曝光了!南通如东一医药企业将槽罐车改成洒水车，夜间偷排高浓度废水...丢大发了!&apos;, 2)the length of text_label 7340the length of train_data 7345 可以看出有5条是id没对应起来，这5条咱就不要了，对大局没影响。 接下来需要对文本做预处理。这里我们做个对比试验，一种是经过预处理的，另一种是未预处理，查看预处理对各种算法的影响。这里需要特别指出的是：现在的深度模型基本不需要对文本做预处理了，很多情况下基于字的特征比词的特征具有更高的准确率，后续小道会针对这个问题作详细的对比实验与说明。但预处理作为NLP的重要基础之一，还是需要了解一下滴本章预处理流程为：分词，去标点，去停用词（也可以将标点写在停用词表内一步到位，这样分开是因为有时候标点和停用词的去除处理不同） 分词（这里使用python最常用的分词包jieba）123import jiebatext_data = [jieba.lcut(str(sentence)) for sentence in text_all]print(text_data[-10:]) 1[[&apos;帮&apos;, &apos;女郎&apos;, &apos;在&apos;, &apos;行动&apos;], [&apos;稻城&apos;, &apos;亚丁&apos;, &apos;+&apos;, &apos;318&apos;, &apos;川藏线&apos;, &apos;+&apos;, &apos;拉萨&apos;, &apos;+&apos;, &apos;青藏线&apos;, &apos;，&apos;, &apos;你&apos;, &apos;心动&apos;, &apos;了&apos;, &apos;吗&apos;, &apos;？&apos;], [&apos;大良&apos;, &apos;、&apos;, &apos;龙江&apos;, &apos;、&apos;, &apos;陈村&apos;, &apos;4&apos;, &apos;所&apos;, &apos;学校&apos;, &apos;9&apos;, &apos;月&apos;, &apos;开学&apos;, &apos;!&apos;, &apos;顺德&apos;, &apos;未来&apos;, &apos;两年&apos;, &apos;力&apos;, &apos;...&apos;], [&apos;最&apos;, &apos;容易&apos;, &apos;受到&apos;, &apos;甲醛&apos;, &apos;侵害&apos;, &apos;的&apos;, &apos;五类&apos;, &apos;人&apos;, &apos;，&apos;, &apos;有&apos;, &apos;你&apos;, &apos;吗&apos;, &apos;？&apos;, &apos;_&apos;, &apos;污染&apos;], [&apos;2019&apos;, &apos;年&apos;, &apos;1&apos;, &apos;月&apos;, &apos;事故&apos;, &apos;伤亡&apos;, &apos;月&apos;, &apos;报&apos;], [&apos;珊瑚&apos;, &apos;裸尾鼠&apos;, &apos;：&apos;, &apos;首个&apos;, &apos;因&apos;, &apos;全球&apos;, &apos;气候&apos;, &apos;变暖&apos;, &apos;灭绝&apos;, &apos;的&apos;, &apos;哺乳动物&apos;], [&apos;独居&apos;, &apos;老人&apos;, &apos;做饭&apos;, &apos;忘关&apos;, &apos;火&apos;, &apos; &apos;, &apos;南通&apos;, &apos;志愿者&apos;, &apos;及时发现&apos;, &apos;转危为安&apos;], [&apos;被&apos;, &apos;生意&apos;, &apos;上&apos;, &apos;的&apos;, &apos;人&apos;, &apos;给&apos;, &apos;利用&apos;, &apos;合同诈骗&apos;, &apos;，&apos;, &apos;诈骗&apos;, &apos;三十万&apos;, &apos;够判&apos;, &apos;多少&apos;, &apos;年&apos;, &apos;-&apos;, &apos;-&apos;, &apos;在&apos;, &apos;..._&apos;, &apos;律师&apos;, &apos;365&apos;], [&apos;奎山&apos;, &apos;汽贸&apos;, &apos;城&apos;, &apos;去年&apos;, &apos;那场&apos;, &apos;火灾&apos;, &apos;，&apos;, &apos;调查&apos;, &apos;情况&apos;, &apos;报告&apos;, &apos;出来&apos;, &apos;了&apos;, &apos;！&apos;], [&apos;曝光&apos;, &apos;台&apos;, &apos;•&apos;, &apos;调查&apos;, &apos;｜&apos;, &apos;市场&apos;, &apos;消防通道&apos;, &apos;被&apos;, &apos;长期&apos;, &apos;霸占&apos;, &apos;？&apos;, &apos;事情&apos;, &apos;并非&apos;, &apos;想象&apos;, &apos;的&apos;, &apos;那样&apos;]] 去除标点符号可以使用string自带的punctuation获取英文标点和zhon包的punctuation获取中文标点 12345678910111213141516171819202122import stringimport zhon.hanzi as hanprint(\"English puctuation is :\",string.punctuation)print(\"Chinese puctuation is :\",han.punctuation)#需要查询操作，所以这里选用查找时间复杂度O(1)的集合punc = set()for i in string.punctuation: punc.add(i)for i in han.punctuation: punc.add(i)#这里举个例子sentence = \"fjdk sal fgjd,.,/,/,/.,.[]\"sentence_after = ''.join(word for word in sentence if word not in punc)print(\"sentence remove punctuation is :\",sentence_after)text_remove_punc = []for sentence in text_data: tmp = [] for word in sentence: if word not in punc: tmp.append(word) text_remove_punc.append(tmp)print(text_remove_punc[:10]) 1234English puctuation is : !&quot;#$%&amp;&apos;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~Chinese puctuation is : ＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､ 、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·！？｡。sentence remove punctuation is : fjdk sal fgjd[[&apos;问责&apos;, &apos;领导&apos;, &apos;上&apos;, &apos;黄镇&apos;, &apos;党委书记&apos;, &apos;张涛&apos;, &apos;宣国&apos;, &apos;才&apos;, &apos;真能&apos;, &apos;一手遮天&apos;, &apos;吗&apos;], [&apos;江歌&apos;, &apos;事件&apos;, &apos;教会&apos;, &apos;孩子&apos;, &apos;善良&apos;, &apos;的&apos;, &apos;同时&apos;, &apos;更要&apos;, &apos;懂得&apos;, &apos;保护&apos;, &apos;自己&apos;], [&apos;绝味&apos;, &apos;鸭&apos;, &apos;脖&apos;, &apos;广告&apos;, &apos;开&apos;, &apos;黄腔&apos;, &apos;引&apos;, &apos;众怒&apos;, &apos; &apos;, &apos;双&apos;, &apos;11&apos;, &apos;这么&apos;, &apos;拼值&apos;, &apos;吗&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;垃圾车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;], [&apos;恶劣&apos;, &apos;至极&apos;, &apos;央视&apos;, &apos;都&apos;, &apos;曝光&apos;, &apos;了&apos;, &apos;南通&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;洒水车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢&apos;, &apos;大发&apos;, &apos;了&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;南通&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢脸&apos;], [&apos;粉丝&apos;, &apos;爆料&apos;, &apos;五洲&apos;, &apos;国际&apos;, &apos;无锡&apos;, &apos;项目&apos;, &apos;涉嫌&apos;, &apos;诈骗&apos;, &apos;非法&apos;, &apos;集资&apos;], [&apos;年内&apos;, &apos;约&apos;, &apos;10&apos;, &apos;起&apos;, &apos;锂电&apos;, &apos;重组&apos;, &apos;失败&apos;, &apos; &apos;, &apos;资本&apos;, &apos;对&apos;, &apos;高&apos;, &apos;估值&apos;, &apos;收购&apos;, &apos;说&apos;, &apos;不&apos;], [&apos;男子&apos;, &apos;梦想&apos;, &apos;一夜&apos;, &apos;暴富&apos;, &apos;持&apos;, &apos;水泥块&apos;, &apos;砸机&apos;], [&apos;北京&apos;, &apos;多家&apos;, &apos;法院&apos;, &apos;供暖&apos;, &apos;纠纷&apos;, &apos;案件&apos;, &apos;主体&apos;, &apos;为&apos;, &apos;供暖费&apos;, &apos;追缴&apos;, &apos;山海&apos;, &apos;网&apos;]] 去除停用词这里提供我自己常用的停用词表，大家可以根据不同的需求自己设定停用词表 123456789101112with open(\"data/stopwords\", encoding='utf-8') as fin: stopwords = set() for i in fin: stopwords.add(i.strip())text_after = []for sentence in text_remove_punc: tmp = [] for word in sentence: if word not in stopwords: tmp.append(word) text_after.append(tmp)print(text_after[:10]) 1[[&apos;问责&apos;, &apos;领导&apos;, &apos;上&apos;, &apos;黄镇&apos;, &apos;党委书记&apos;, &apos;张涛&apos;, &apos;宣国&apos;, &apos;才&apos;, &apos;真能&apos;, &apos;一手遮天&apos;, &apos;吗&apos;], [&apos;江歌&apos;, &apos;事件&apos;, &apos;教会&apos;, &apos;孩子&apos;, &apos;善良&apos;, &apos;的&apos;, &apos;同时&apos;, &apos;更要&apos;, &apos;懂得&apos;, &apos;保护&apos;, &apos;自己&apos;], [&apos;绝味&apos;, &apos;鸭&apos;, &apos;脖&apos;, &apos;广告&apos;, &apos;开&apos;, &apos;黄腔&apos;, &apos;引&apos;, &apos;众怒&apos;, &apos; &apos;, &apos;双&apos;, &apos;11&apos;, &apos;这么&apos;, &apos;拼值&apos;, &apos;吗&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;垃圾车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;], [&apos;恶劣&apos;, &apos;至极&apos;, &apos;央视&apos;, &apos;都&apos;, &apos;曝光&apos;, &apos;了&apos;, &apos;南通&apos;, &apos;如东&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;将&apos;, &apos;槽罐车&apos;, &apos;改成&apos;, &apos;洒水车&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢&apos;, &apos;大发&apos;, &apos;了&apos;], [&apos;央视&apos;, &apos;曝光&apos;, &apos;南通&apos;, &apos;一&apos;, &apos;医药企业&apos;, &apos;夜间&apos;, &apos;偷排&apos;, &apos;高浓度&apos;, &apos;废水&apos;, &apos;...&apos;, &apos;丢脸&apos;], [&apos;粉丝&apos;, &apos;爆料&apos;, &apos;五洲&apos;, &apos;国际&apos;, &apos;无锡&apos;, &apos;项目&apos;, &apos;涉嫌&apos;, &apos;诈骗&apos;, &apos;非法&apos;, &apos;集资&apos;], [&apos;年内&apos;, &apos;约&apos;, &apos;10&apos;, &apos;起&apos;, &apos;锂电&apos;, &apos;重组&apos;, &apos;失败&apos;, &apos; &apos;, &apos;资本&apos;, &apos;对&apos;, &apos;高&apos;, &apos;估值&apos;, &apos;收购&apos;, &apos;说&apos;, &apos;不&apos;], [&apos;男子&apos;, &apos;梦想&apos;, &apos;一夜&apos;, &apos;暴富&apos;, &apos;持&apos;, &apos;水泥块&apos;, &apos;砸机&apos;], [&apos;北京&apos;, &apos;多家&apos;, &apos;法院&apos;, &apos;供暖&apos;, &apos;纠纷&apos;, &apos;案件&apos;, &apos;主体&apos;, &apos;为&apos;, &apos;供暖费&apos;, &apos;追缴&apos;, &apos;山海&apos;, &apos;网&apos;]] Word Embedding然后将其Embedding，就是用向量表示文本。在word2vec以前，大都使用one-hot加tf-idf来向量化文本 12 12 12 12 12 12 12 然后划分训练集和验证集，这里按照9:1划分，由于没有测试集，这里小道用验证集当做测试集来计算得分。这里可以直接调用sklearn的划分数据集的函数，或者自己写一个划分函数，既然写到这儿了，我就自己写一个，顺便测测和sklearn划分函数的时间差距。 1234567891011121314151617181920212223242526272829303132333435363738#整体思路是先把索引random，然后按比例向下取整选取。import numpy as npimport osimport mathimport randomimport timedef heng_split(filein, fileout_dir, train_persent): is_exsist = os.path.exists(fileout_dir) if not is_exsist: os.mkdir(fileout_dir) else: print(\"%s目录已经存在\"%(fileout_dir)) with open(fileout_dir+\"/train.csv\", 'w', encoding='utf-8') as fout_train: with open(fileout_dir+\"/test.csv\", 'w', encoding='utf-8') as fout_test: with open(filein, encoding='utf-8') as fin: content = fin.readlines() count_line = len(content) before = [k for k in range(count_line)] random.shuffle(before) content =[content[i] for i in before] train_count = int(math.floor(count_line * train_persent)) for index, i in enumerate(content): if index &lt;= train_count: fout_train.write(i) else: fout_test.write(i)def main(): filein = \"train.tsv\" # for train_persent in np.arange(0.6,0.95,0.05): for train_persent in [0.95]: fileout_dir = \"out\" + str(train_persent)[:4] heng_split(filein, fileout_dir, train_persent)if __name__ == \"__main__\": main() #这里是sklearn的划分方法from sklearn.model_selection import train_test_split if name == “main“: data = pd.read_csv(“G:/dataset/wine.csv”) #将样本分为x表示特征，y表示类别 x,y = data.ix[:,1:],data.ix[:,0] #测试集为30%，训练集为70% x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0) print(len(x_train)) #124 print(len(x_test)) 12","link":"/2019/10/23/文本分类/"},{"title":"聚类算法详解-从文本聚类谈起","text":"无监督的将相似句聚类，所以需要对聚类算法有比较深入的研究，单纯的调用sklearn无法满足工作需求，故对其进行实现。本文主要分三部分，第一部分描述对层次聚类的总结(因为这是工作中最终选取的算法)，第二部分描述kmeans算法(谈聚类怎么能不谈kmeans)，第三部分描述神经网络的聚类算法 算法详解层次聚类的步骤，终止条件分为两种，一种是聚到k类停止，一种是每两类之间的阈值大于某数。由于sklearn已经实现第一种方法，这里就不在赘述，主要分析一下第二种终止条件的实现及遇到的一些问题，全部代码请参考我的github。 算法原理层次聚类，顾名思义是将数据点分层进行聚类，主要有两种思路，一种是凝聚法，即算法开始时将每个节点设为一类，然后按照类间距离开始凝聚，知道凝聚满足终止条件。另一种是分裂法，即算法开始时所有数据点视为一类，然后开始分裂，直到分裂满足终止条件。 这里以凝聚法为例（分裂法原理一样，只是刚好相反），层次聚类涉及的主要问题在于类别之间的距离计算及聚类中心点的选择。 距离计算公式其他的距离计算公式在这里也不赘述，主流的距离度量方法可以参考sklearn的实现，sklearn的凝聚法实现了有6种距离度量，分别为“euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or ‘precomputed’。 由于我们是文本聚类，所以选择文本中最常用的余弦距离（选择余弦是有道理的，请自行google，或者等我填坑。。。）。 聚类中心点选择这里选择平均距离，因为经测试后，这种距离效果最好。 代码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215#层次聚类，两类之间使用平均距离作为类间距离from sklearn import preprocessingimport numpy as npimport heng_hierarchical_pre as h_preimport timeclass Hierarchical(object): \"\"\" 层次聚类 :param: min_dist -&gt;float32 聚类终止条件 :param: sub_node_id -&gt;list 存放聚完类的节点 :param: point_num -&gt;int 节点个数 :param: feature_num -&gt;int 节点特征数 \"\"\" def __init__(self, min_dist=0.63): self.min_dist = min_dist self.point_num = 0 self.feature_num = 0 self.distance_ori = None def binary_search1(self,nodes_pair_sort, x): ''' 二分查找 ''' left, right = 0, len(nodes_pair_sort)-1 if x&gt;=nodes_pair_sort[0]: return 0 if x&lt;=nodes_pair_sort[-1]: return -1 while left&lt;right: mid = int((left + right) / 2) if nodes_pair_sort[mid] &lt; x: right = mid if nodes_pair_sort[mid-1] &gt;= x: return mid-1 else: left = mid + 1 if nodes_pair_sort[mid+1] &lt;= x: return mid def binary_search2(self,nodes_pair_sort, x): ''' 二分查找 ''' left, right = 0, len(nodes_pair_sort)-1 if x&gt;=nodes_pair_sort[0][2]: return 0 if x&lt;=nodes_pair_sort[-1][2]: return -1 while left&lt;right: mid = int((left + right) / 2) if nodes_pair_sort[mid][2] &lt; x: right = mid if nodes_pair_sort[mid-1][2] &gt;= x: return mid else: left = mid + 1 if nodes_pair_sort[mid+1][2] &lt;= x: return mid+1 def fit(self, data): \"\"\" 训练模型，得到最终聚类结果 :param : data -&gt;np.array() 传入的训练数据 :param : distance_final -&gt;list 满足约束条件的节点对组成的列表 \"\"\" nodes = {} nodes_all_between_dist = {} data = np.array(data) self.point_num, self.feature_num = np.shape(data)[0], np.shape(data)[1] data = preprocessing.normalize(data,'l2') distance_big = np.zeros([self.point_num*2,self.point_num*2]) distance_ori = np.dot(data, data.T) self.distance_ori = distance_ori for i in range(self.point_num): distance_ori[i][i]=0 distance_half = np.triu(distance_ori) for i in range(self.point_num): nodes[i] = [i] nodes_all_between_dist[i] = distance_ori[i] min_dist = self.min_dist index_min_dist = np.where(distance_half &gt;= min_dist) nodes_pair = list()#存放全部满足条件节点对(left,right,dist) node_id_new = self.point_num#新节点id #满足条件节点对 for i in range(len(index_min_dist[0])): raw_distance, colume_distance = index_min_dist[0][i], index_min_dist[1][i] nodes_pair.append((raw_distance,colume_distance,distance_half[raw_distance][colume_distance])) nodes_before_del = set() nodes_pair_sort_1 = sorted(nodes_pair, key=lambda x:x[2], reverse=True) len_nodes_pair_sort = len(nodes_pair_sort_1) #每1000个放一个列表noedes_pair_sort = [[],[],...] print('排序前列表长度%d'%(len_nodes_pair_sort)) nodes_pair_sort = [] nodes_pair_range = np.arange(0, len_nodes_pair_sort, 16000) for index, value in enumerate(nodes_pair_range[:-1]): nodes_pair_sort.append(nodes_pair_sort_1[nodes_pair_range[index]:nodes_pair_range[index+1]]) nodes_pair_sort.append(nodes_pair_sort_1[nodes_pair_range[-1]:len_nodes_pair_sort]) count=0 distance_big[:self.point_num, :self.point_num] = distance_ori print('排序后列表长度%d'%(len(nodes_pair_sort))) while len(nodes_pair_sort)&gt;1: # start1 = time.time() # np.sort() # start2 = time.time() # print('排序耗时%f'%(start2-start1)) if nodes_pair_sort[0] == []: del nodes_pair_sort[0] #将每个组的最大的数加入nodes_pair_range_head中 nodes_pair_range_head = [] for value in nodes_pair_sort: # print(value) nodes_pair_range_head.append(value[0][2]) left, right = nodes_pair_sort[0][0][0], nodes_pair_sort[0][0][1] if left in nodes_before_del or right in nodes_before_del: del nodes_pair_sort[0][0] else: count+=1 nodes_before_del.add(left) nodes_before_del.add(right) # print('聚类的左右节点为%s,%s\\n'%(left,right)) # file_test.write('聚类的左右节点为%s,%s\\n'%(left,right)) nodes[node_id_new] = [] nodes[node_id_new] += nodes[left] nodes[node_id_new] += nodes[right] # print('新节点及其子节点为%s,%s'%(node_id_new,nodes[node_id_new])) start_insert = time.time() new_distance_1 = (distance_big[left]*len(nodes[left]) + distance_big[right]*len(nodes[right])) new_distance = new_distance_1/(len(nodes[left])+len(nodes[right])) # print(new_distance.shape) distance_big[:,node_id_new] = new_distance # print(distance_ori.shape) distance_big[node_id_new,:] = new_distance.reshape(1,len(new_distance)) end_insert = time.time() # print('计算节点耗时%f'%(end_insert - start_insert)) # print(distance_ori.shape) # print('节点有:%s'%' '.join([str(i) for i in nodes])) start_out = time.time() for node in nodes_before_del: distance_big[node,node_id_new] = 0 distance_big[node_id_new,node] = 0 nodes.pop(left) nodes.pop(right) node_new_dist = distance_big[:,node_id_new] count_insert = 0 start_getNewdict = time.time() for count_1,i in enumerate(node_new_dist): if i &gt;= min_dist: count_insert += 1 # print(nodes_pair_range_head) # print(len(nodes_pair_range_head)) nodes_pair_index = self.binary_search1(nodes_pair_range_head, i) # print(nodes_pair_index, i) index = self.binary_search2(nodes_pair_sort[nodes_pair_index], i) nodes_pair_sort[nodes_pair_index].insert(index,(node_id_new,count_1,i)) end_getNewdict = time.time() # print('查找插入索引耗时%f'%(end_getNewdict - start_getNewdict)) # print('插入次数%d'%count_insert) node_id_new += 1 end_out = time.time() # print('循环总耗时%f'%(end_out-start_out)) start_insert = time.time() labels = [nodes[i] for i in nodes] print('排序类别为%s'%len(nodes)) temp_labels = [] for index, value in enumerate(labels): if len(value)&gt;1: for j in value: temp_labels.append((j,index)) else: temp_labels.append((value[0],index)) labels = sorted(temp_labels,key=lambda x:x[0]) labels = [j for i,j in labels] end_insert = time.time() # print('排序节点耗时%f'%(end_insert - start_insert)) print('排序次数%d'%count) return labels def main(): start1 = time.time() data,_ = h_pre.main('udesk_test2.txt') print(data.shape) start2 = time.time() print('读取词向量耗时%.6s秒'%str(start2-start1)) print('*'*40) my1 = time.time() clust = Hierarchical(0.6) clust.fit(data) end1 = time.time() print('程序耗时%.6f秒'%float(end1-my1))if __name__ == '__main__': main()","link":"/2018/09/01/浅谈文本聚类/"}],"tags":[{"name":"nlp","slug":"nlp","link":"/tags/nlp/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"深度学习","slug":"深度学习","link":"/categories/深度学习/"},{"name":"自然语言理解","slug":"自然语言理解","link":"/categories/自然语言理解/"},{"name":"机器学习","slug":"机器学习","link":"/categories/机器学习/"}]}